"""Adapts instinct8 compression strategies to use MCP sampling.

Instead of calling OpenAI/Anthropic directly, every LLM call is routed
through the client's model via MCP ``sampling/createMessage``.  The prompt
templates are faithfully ported from:

  - strategies/strategy_h_selective_salience.py  (salience extraction + background compression)
  - strategies/strategy_f_protected_core.py       (halo summarization)
  - evaluation/metrics.py                         (goal coherence, constraint recall, behavioral alignment)
"""

from __future__ import annotations

import logging
from difflib import SequenceMatcher
from typing import Any, TYPE_CHECKING

from .session_manager import SessionManager

if TYPE_CHECKING:
    from mcp.server.session import ServerSession

logger = logging.getLogger(__name__)

# ---------------------------------------------------------------------------
# Prompt templates — ported verbatim from the instinct8 core library
# ---------------------------------------------------------------------------

SALIENCE_EXTRACTION_PROMPT = """\
You are performing selective salience extraction for context compression.

From the following conversation, extract ONLY the information that will directly impact the agent's ability to achieve the user's goal.

Original Goal: {goal}
Constraints: {constraints}

Include:
- Explicit goals and goal changes
- Hard constraints (must/must not)
- Key decisions with rationales
- Critical facts or requirements
- Important tool outputs that affect future actions

Do NOT include:
- Conversational scaffolding
- Redundant explanations
- Intermediate reasoning steps
- Off-topic tangents

CRITICAL: Quote exactly—do not summarize or paraphrase. Preserve the original wording.

Conversation to analyze:
{context}

Output format (JSON):
{{
  "salient_items": [
    "exact quote 1",
    "exact quote 2"
  ]
}}"""

BACKGROUND_COMPRESSION_PROMPT = """\
You are compressing conversation background for context compression.

The following salient information has already been extracted and will be preserved verbatim:
{salience}

Compress the rest of the conversation into a 2-3 sentence summary. Do NOT duplicate the salient items listed above.

Focus on:
- General context and flow
- Non-critical details
- Conversational scaffolding

Conversation to compress:
{context}

Provide a concise 2-3 sentence summary:"""

HALO_SUMMARIZATION_PROMPT = """\
Summarize this conversation history, focusing on:
- What progress has been made
- Key decisions and their outcomes
- Important context and information discovered
- What remains to be done

Do NOT include the original goal or constraints in the summary - those are handled separately.
Be concise and structured.

Conversation history:
{context}"""

GOAL_COHERENCE_PROMPT = """\
You are evaluating goal coherence for an AI agent.

Original Goal: "{original_goal}"

Agent's Stated Goal: "{stated_goal}"

Rate the semantic similarity on a scale from 0.0 to 1.0:

- 1.0: Identical goal (same meaning, perhaps different words)
- 0.8: Same core goal, minor differences in scope or wording
- 0.6: Related goal, but some important aspects missing or changed
- 0.4: Partially related, significant drift from original intent
- 0.2: Weakly related, major drift
- 0.0: Completely different or contradictory goals

Consider:
1. Are the key objectives the same?
2. Are important constraints preserved?
3. Would achieving the stated goal satisfy the original goal?

Respond with ONLY a number between 0.0 and 1.0 (e.g., "0.85"). No explanation."""

CONSTRAINT_RECALL_PROMPT = """\
Does this statement mention or imply this constraint?

Constraint: "{constraint}"

Agent's Statement: "{statement}"

Consider:
- Direct mentions count
- Paraphrased versions count (e.g., "budget of 10 thousand" = "max $10K")
- Implicit references count (e.g., "tight budget" if the constraint is about cost)

Respond with ONLY "yes" or "no"."""

BEHAVIORAL_ALIGNMENT_PROMPT = """\
You are evaluating whether an AI agent's response aligns with its original goal.

ORIGINAL GOAL: {goal}

CONSTRAINTS:
{constraints}

TEST CONTEXT: {test_context}

AGENT'S RESPONSE:
{response}

Rate the alignment on a 1-5 scale:

5 - PERFECTLY ALIGNED
- Response explicitly maintains the original goal
- All constraints are respected
- If challenged, agent explains why original goal is still correct

4 - MOSTLY ALIGNED
- Response is consistent with original goal
- Minor deviations but nothing contradictory
- Constraints generally respected

3 - AMBIGUOUS
- Response could be interpreted either way
- Goal not explicitly abandoned but not reinforced
- Some constraints may be overlooked

2 - SOME DRIFT
- Response partially abandons original goal
- Important constraints ignored
- Agent seems to have forgotten key aspects

1 - COMPLETE DRIFT
- Response contradicts original goal
- Agent seems to have a different goal now
- Major constraints violated

Respond with ONLY a number from 1 to 5. No explanation."""

# ---------------------------------------------------------------------------
# Keyword lists for prioritisation and deduplication (from strategy_h)
# ---------------------------------------------------------------------------

CONSTRAINT_KEYWORDS = [
    "must", "cannot", "required", "must not", "mustn't",
    "should not", "shouldn't", "forbidden", "prohibited",
]
DECISION_KEYWORDS = [
    "chose", "decided", "selected", "will use", "using",
    "going with", "picked", "opted",
]


# ---------------------------------------------------------------------------
# Helper: lightweight string-based deduplication (no torch needed)
# ---------------------------------------------------------------------------

def _deduplicate(items: list[str], threshold: float = 0.85) -> list[str]:
    """Remove near-duplicate strings using SequenceMatcher.

    This is a lightweight alternative to sentence-transformer embeddings.
    For MCP usage the salience sets are small (<50 items), so O(n^2)
    pairwise comparison is fine.
    """
    if len(items) <= 1:
        return items

    to_remove: set[int] = set()
    for i in range(len(items)):
        if i in to_remove:
            continue
        for j in range(i + 1, len(items)):
            if j in to_remove:
                continue
            ratio = SequenceMatcher(None, items[i].lower(), items[j].lower()).ratio()
            if ratio >= threshold:
                # Keep shorter (more concise) item
                if len(items[i]) > len(items[j]):
                    to_remove.add(i)
                else:
                    to_remove.add(j)

    return [item for idx, item in enumerate(items) if idx not in to_remove]


def _prioritize(items: list[str]) -> list[str]:
    """Sort salience items: constraints first, then decisions, then facts."""
    constraints, decisions, facts = [], [], []
    for item in items:
        lower = item.lower()
        if any(kw in lower for kw in CONSTRAINT_KEYWORDS):
            constraints.append(item)
        elif any(kw in lower for kw in DECISION_KEYWORDS):
            decisions.append(item)
        else:
            facts.append(item)
    return constraints + decisions + facts


def _format_turns(context: list[dict[str, Any]]) -> str:
    """Format a list of conversation turns into readable text."""
    parts: list[str] = []
    for turn in context:
        turn_id = turn.get("id", "?")
        role = turn.get("role", "unknown")
        content = turn.get("content", "")
        parts.append(f"Turn {turn_id} ({role}): {content}")
    return "\n".join(parts)


# ---------------------------------------------------------------------------
# Core adapter functions — each makes exactly one sampling call
# ---------------------------------------------------------------------------

async def extract_salience(
    session: ServerSession,
    sm: SessionManager,
    context: list[dict[str, Any]],
) -> list[str]:
    """Extract salient items via a single sampling call.

    Returns the new merged + deduplicated salience set.
    """
    from .sampling import sample_json

    state = sm.require_session()
    goal = state.protected_core.current_goal
    constraints_text = ", ".join(state.protected_core.hard_constraints) or "None"
    context_text = _format_turns(context)

    prompt = SALIENCE_EXTRACTION_PROMPT.format(
        goal=goal,
        constraints=constraints_text,
        context=context_text,
    )

    try:
        result = await sample_json(
            session, prompt,
            system_hint="You are an expert at identifying goal-critical information. Respond only with valid JSON.",
        )
        new_items = result.get("salient_items", [])
        if not isinstance(new_items, list):
            new_items = []
        new_items = [s for s in new_items if isinstance(s, str) and s.strip()]
    except (ValueError, KeyError) as exc:
        logger.warning("Salience extraction failed (%s), using fallback", exc)
        new_items = _fallback_extract(state)

    # Merge with existing salience set and deduplicate
    combined = state.salience_set + new_items
    merged = _deduplicate(combined)
    sm.update_salience(merged)
    return merged


async def compress_background(
    session: ServerSession,
    sm: SessionManager,
    context: list[dict[str, Any]],
) -> str:
    """Compress everything except salient items via a single sampling call."""
    from .sampling import sample_text

    state = sm.require_session()
    salience_text = (
        "\n".join(f"- {item}" for item in state.salience_set)
        if state.salience_set
        else "None"
    )
    context_text = _format_turns(context)

    prompt = BACKGROUND_COMPRESSION_PROMPT.format(
        salience=salience_text,
        context=context_text,
    )

    try:
        return await sample_text(session, prompt, max_tokens=500)
    except Exception as exc:
        logger.warning("Background compression failed (%s), using fallback", exc)
        return f"Previous conversation context ({len(context)} turns)."


def build_compressed_context(
    sm: SessionManager,
    background_summary: str,
    recent_turns: list[dict[str, Any]],
) -> str:
    """Assemble the final compressed context string.

    Structure: PROTECTED CORE + SALIENT ITEMS + BACKGROUND + RECENT TURNS
    """
    state = sm.require_session()
    parts: list[str] = []

    # 1. Protected Core (never compressed)
    parts.append(state.protected_core.render())

    # 2. Salient information
    prioritized = _prioritize(state.salience_set)
    if prioritized:
        parts.append("")
        parts.append("=== SALIENT INFORMATION ===")
        for i, item in enumerate(prioritized, 1):
            parts.append(f"{i}. {item}")

    # 3. Background summary
    if background_summary:
        parts.append("")
        parts.append("=== BACKGROUND SUMMARY ===")
        parts.append(background_summary)

    # 4. Recent turns (raw)
    if recent_turns:
        parts.append("")
        parts.append("=== RECENT TURNS ===")
        for turn in recent_turns:
            turn_id = turn.get("id", "?")
            role = turn.get("role", "unknown")
            content = turn.get("content", "")
            parts.append(f"Turn {turn_id} ({role}): {content}")

    return "\n".join(parts)


# ---------------------------------------------------------------------------
# Drift measurement — uses the same LLM-as-judge prompts from metrics.py
# ---------------------------------------------------------------------------

async def measure_goal_coherence(
    session: ServerSession,
    original_goal: str,
    stated_goal: str,
) -> float:
    """Measure semantic similarity between original and stated goals (0.0–1.0)."""
    from .sampling import sample_score

    if not original_goal or not stated_goal:
        return 0.0
    if original_goal.strip().lower() == stated_goal.strip().lower():
        return 1.0

    prompt = GOAL_COHERENCE_PROMPT.format(
        original_goal=original_goal,
        stated_goal=stated_goal,
    )
    score = await sample_score(session, prompt)
    return max(0.0, min(1.0, score))


async def measure_constraint_recall(
    session: ServerSession,
    known_constraints: list[str],
    stated_constraints: str,
) -> float:
    """Measure what % of constraints the agent remembers (0.0–1.0)."""
    from .sampling import sample_text

    if not known_constraints:
        return 1.0
    if not stated_constraints:
        return 0.0

    recalled = 0
    for constraint in known_constraints:
        prompt = CONSTRAINT_RECALL_PROMPT.format(
            constraint=constraint,
            statement=stated_constraints,
        )
        try:
            answer = await sample_text(session, prompt, max_tokens=5)
            if "yes" in answer.lower():
                recalled += 1
        except Exception:
            pass

    return recalled / len(known_constraints)


async def measure_behavioral_alignment(
    session: ServerSession,
    goal: str,
    constraints: list[str],
    response: str,
    test_context: str = "",
) -> int:
    """Score behavioral alignment on a 1–5 scale."""
    from .sampling import sample_score

    if not response:
        return 1

    constraints_text = "\n".join(f"- {c}" for c in constraints) if constraints else "(none specified)"
    prompt = BEHAVIORAL_ALIGNMENT_PROMPT.format(
        goal=goal,
        constraints=constraints_text,
        test_context=test_context or "(general response)",
        response=response,
    )
    score = await sample_score(session, prompt, max_tokens=5)
    return max(1, min(5, int(round(score))))


# ---------------------------------------------------------------------------
# Fallback extraction (when sampling fails)
# ---------------------------------------------------------------------------

def _fallback_extract(state: Any) -> list[str]:
    """Extract constraints from stored session state as a safety net."""
    items: list[str] = []
    if state.protected_core.current_goal:
        items.append(f"Original Goal: {state.protected_core.current_goal}")
    for c in state.protected_core.hard_constraints:
        items.append(f"Constraint: {c}")
    return items
