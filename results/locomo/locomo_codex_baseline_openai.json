{
  "agent_name": "CodexAgent(Strategy B - Codex-Style Checkpoint)",
  "strategy_name": "Strategy B - Codex-Style Checkpoint",
  "model": "gpt-4o-mini",
  "dataset_path": "/Users/loganmay/My Applications/Gauntlet/instinct8/main/A-mem/data/locomo10.json",
  "total_questions": 199,
  "category_distribution": {
    "2": 37,
    "3": 13,
    "1": 32,
    "4": 70,
    "5": 47
  },
  "aggregate_metrics": {
    "overall": {
      "exact_match": {
        "mean": 0.1658291457286432,
        "std": 0.37286522459448107,
        "median": 0,
        "min": 0,
        "max": 1,
        "count": 199
      },
      "f1": {
        "mean": 0.40869249146339653,
        "std": 0.3650811976101645,
        "median": 0.3333333333333333,
        "min": 0.0,
        "max": 1.0,
        "count": 199
      },
      "rouge1_f": {
        "mean": 0.41764696253660116,
        "std": 0.366827035426299,
        "median": 0.3333333333333333,
        "min": 0.0,
        "max": 1.0,
        "count": 199
      },
      "rouge2_f": {
        "mean": 0.31369603719097033,
        "std": 0.3842109509495692,
        "median": 0.08333333333333333,
        "min": 0.0,
        "max": 1.0,
        "count": 199
      },
      "rougeL_f": {
        "mean": 0.4107408987015794,
        "std": 0.3677899228428379,
        "median": 0.3333333333333333,
        "min": 0.0,
        "max": 1.0,
        "count": 199
      },
      "bleu1": {
        "mean": 0.3459380203143085,
        "std": 0.3525442123909834,
        "median": 0.2222222222222222,
        "min": 0,
        "max": 1.0,
        "count": 199
      },
      "bleu2": {
        "mean": 0.2842991176017672,
        "std": 0.35727145532006543,
        "median": 0.09365858115816939,
        "min": 0,
        "max": 1.0,
        "count": 199
      },
      "bleu3": {
        "mean": 0.2468210042907972,
        "std": 0.345444292555589,
        "median": 0.06665016134041894,
        "min": 0,
        "max": 1.0,
        "count": 199
      },
      "bleu4": {
        "mean": 0.20669056408842482,
        "std": 0.3163881372279554,
        "median": 0.05372849659117709,
        "min": 0,
        "max": 1.0,
        "count": 199
      },
      "bert_precision": {
        "mean": 0.8986025302853416,
        "std": 0.06155176341893277,
        "median": 0.8836987018585205,
        "min": 0.7658332586288452,
        "max": 1.000000238418579,
        "count": 199
      },
      "bert_recall": {
        "mean": 0.902815808602913,
        "std": 0.06282544416205983,
        "median": 0.8904045820236206,
        "min": 0.734740138053894,
        "max": 1.000000238418579,
        "count": 199
      },
      "bert_f1": {
        "mean": 0.9002210659597387,
        "std": 0.05892877750454973,
        "median": 0.8888671398162842,
        "min": 0.7939960956573486,
        "max": 1.000000238418579,
        "count": 199
      },
      "meteor": {
        "mean": 0.3554245261018673,
        "std": 0.3771969099827344,
        "median": 0.17543859649122806,
        "min": 0.0,
        "max": 0.9998177842565598,
        "count": 199
      },
      "sbert_similarity": {
        "mean": 0.58768332323256,
        "std": 0.3092489563181031,
        "median": 0.6080063581466675,
        "min": -0.0609419085085392,
        "max": 1.000000238418579,
        "count": 199
      }
    },
    "category_1": {
      "exact_match": {
        "mean": 0,
        "std": 0.0,
        "median": 0.0,
        "min": 0,
        "max": 0,
        "count": 32
      },
      "f1": {
        "mean": 0.18157326074138838,
        "std": 0.22534928412017802,
        "median": 0.1359447004608295,
        "min": 0.0,
        "max": 0.8,
        "count": 32
      },
      "rouge1_f": {
        "mean": 0.18085344260101238,
        "std": 0.22374441997100472,
        "median": 0.12662013958125623,
        "min": 0.0,
        "max": 0.8,
        "count": 32
      },
      "rouge2_f": {
        "mean": 0.05577793916609706,
        "std": 0.15774020021043608,
        "median": 0.0,
        "min": 0.0,
        "max": 0.6666666666666666,
        "count": 32
      },
      "rougeL_f": {
        "mean": 0.16445847241896594,
        "std": 0.20794792435906725,
        "median": 0.10084745762711865,
        "min": 0.0,
        "max": 0.8,
        "count": 32
      },
      "bleu1": {
        "mean": 0.12408529758186874,
        "std": 0.1440187535704902,
        "median": 0.101010101010101,
        "min": 0,
        "max": 0.6666666666666666,
        "count": 32
      },
      "bleu2": {
        "mean": 0.061437517101346845,
        "std": 0.11181471498588481,
        "median": 0.029509323958628826,
        "min": 0,
        "max": 0.5773502691896257,
        "count": 32
      },
      "bleu3": {
        "mean": 0.036738654013410976,
        "std": 0.06192431649772452,
        "median": 0.01820824295523018,
        "min": 0,
        "max": 0.3254992452916931,
        "count": 32
      },
      "bleu4": {
        "mean": 0.029522491527061875,
        "std": 0.04703632670145217,
        "median": 0.012889683282629495,
        "min": 0,
        "max": 0.24028114141347542,
        "count": 32
      },
      "bert_precision": {
        "mean": 0.8497747872024775,
        "std": 0.037761703678672945,
        "median": 0.8452618718147278,
        "min": 0.783317506313324,
        "max": 0.9506014585494995,
        "count": 32
      },
      "bert_recall": {
        "mean": 0.8506757654249668,
        "std": 0.03460432120010089,
        "median": 0.846091240644455,
        "min": 0.7936490774154663,
        "max": 0.9506014585494995,
        "count": 32
      },
      "bert_f1": {
        "mean": 0.8497299775481224,
        "std": 0.02965223108054011,
        "median": 0.8419777750968933,
        "min": 0.8093538284301758,
        "max": 0.9506014585494995,
        "count": 32
      },
      "meteor": {
        "mean": 0.07637843858345567,
        "std": 0.18765521829695894,
        "median": 0.0,
        "min": 0.0,
        "max": 0.8928571428571429,
        "count": 32
      },
      "sbert_similarity": {
        "mean": 0.4334766912506893,
        "std": 0.2191769347152253,
        "median": 0.4402535408735275,
        "min": 0.028898779302835464,
        "max": 0.8755380511283875,
        "count": 32
      }
    },
    "category_2": {
      "exact_match": {
        "mean": 0.05405405405405406,
        "std": 0.22924343513512563,
        "median": 0,
        "min": 0,
        "max": 1,
        "count": 37
      },
      "f1": {
        "mean": 0.44994539994539995,
        "std": 0.2923710959252662,
        "median": 0.5,
        "min": 0.0,
        "max": 1.0,
        "count": 37
      },
      "rouge1_f": {
        "mean": 0.4493447993447993,
        "std": 0.2928742804312778,
        "median": 0.5,
        "min": 0.0,
        "max": 1.0,
        "count": 37
      },
      "rouge2_f": {
        "mean": 0.30330330330330335,
        "std": 0.31394178408988355,
        "median": 0.28571428571428575,
        "min": 0.0,
        "max": 1.0,
        "count": 37
      },
      "rougeL_f": {
        "mean": 0.4493447993447993,
        "std": 0.2928742804312778,
        "median": 0.5,
        "min": 0.0,
        "max": 1.0,
        "count": 37
      },
      "bleu1": {
        "mean": 0.32008444015752757,
        "std": 0.2513721570081917,
        "median": 0.3333333333333333,
        "min": 0,
        "max": 1.0,
        "count": 37
      },
      "bleu2": {
        "mean": 0.21145736898716264,
        "std": 0.24325149173500027,
        "median": 0.12909944487358058,
        "min": 0,
        "max": 1.0,
        "count": 37
      },
      "bleu3": {
        "mean": 0.1582328197727963,
        "std": 0.1979035831529073,
        "median": 0.09635409769034631,
        "min": 0,
        "max": 1.0,
        "count": 37
      },
      "bleu4": {
        "mean": 0.10778502940169976,
        "std": 0.11189648509447526,
        "median": 0.09554427922043669,
        "min": 0,
        "max": 0.5623413251903491,
        "count": 37
      },
      "bert_precision": {
        "mean": 0.919406370536701,
        "std": 0.050093339564120855,
        "median": 0.9270618557929993,
        "min": 0.7658332586288452,
        "max": 1.0,
        "count": 37
      },
      "bert_recall": {
        "mean": 0.9023320868208602,
        "std": 0.05552250739916474,
        "median": 0.8871561288833618,
        "min": 0.8029053211212158,
        "max": 1.0,
        "count": 37
      },
      "bert_f1": {
        "mean": 0.9100302779996717,
        "std": 0.046362997303062196,
        "median": 0.907796323299408,
        "min": 0.8097864389419556,
        "max": 1.0,
        "count": 37
      },
      "meteor": {
        "mean": 0.26419414767353033,
        "std": 0.26324402858087,
        "median": 0.17241379310344826,
        "min": 0.0,
        "max": 0.9814814814814815,
        "count": 37
      },
      "sbert_similarity": {
        "mean": 0.729531592613942,
        "std": 0.19363189003627218,
        "median": 0.7955406308174133,
        "min": 0.3672371506690979,
        "max": 1.000000238418579,
        "count": 37
      }
    },
    "category_3": {
      "exact_match": {
        "mean": 0,
        "std": 0.0,
        "median": 0,
        "min": 0,
        "max": 0,
        "count": 13
      },
      "f1": {
        "mean": 0.07431087431087431,
        "std": 0.12049926164649141,
        "median": 0.0,
        "min": 0.0,
        "max": 0.3703703703703704,
        "count": 13
      },
      "rouge1_f": {
        "mean": 0.07690143190143191,
        "std": 0.1194681970786976,
        "median": 0.0,
        "min": 0.0,
        "max": 0.35714285714285715,
        "count": 13
      },
      "rouge2_f": {
        "mean": 0.0059171597633136085,
        "std": 0.02133462293173958,
        "median": 0.0,
        "min": 0.0,
        "max": 0.07692307692307691,
        "count": 13
      },
      "rougeL_f": {
        "mean": 0.07690143190143191,
        "std": 0.1194681970786976,
        "median": 0.0,
        "min": 0.0,
        "max": 0.35714285714285715,
        "count": 13
      },
      "bleu1": {
        "mean": 0.08733574118189502,
        "std": 0.11229160490608991,
        "median": 0.047619047619047616,
        "min": 0,
        "max": 0.3333333333333333,
        "count": 13
      },
      "bleu2": {
        "mean": 0.03155819024786693,
        "std": 0.0540452690067173,
        "median": 0.01543033499620919,
        "min": 0,
        "max": 0.19802950859533483,
        "count": 13
      },
      "bleu3": {
        "mean": 0.016307914365466247,
        "std": 0.020095552002994593,
        "median": 0.011280777824436245,
        "min": 0,
        "max": 0.06434016142474894,
        "count": 13
      },
      "bleu4": {
        "mean": 0.012015675822923932,
        "std": 0.013582303407772976,
        "median": 0.00913442366656447,
        "min": 0,
        "max": 0.039281465090051315,
        "count": 13
      },
      "bert_precision": {
        "mean": 0.854846518773299,
        "std": 0.030116922186402946,
        "median": 0.8653685450553894,
        "min": 0.7937987446784973,
        "max": 0.8946107029914856,
        "count": 13
      },
      "bert_recall": {
        "mean": 0.8629397199704096,
        "std": 0.024234639723460717,
        "median": 0.8560183048248291,
        "min": 0.8192214965820312,
        "max": 0.8911182880401611,
        "count": 13
      },
      "bert_f1": {
        "mean": 0.8586845168700585,
        "std": 0.024027732379487063,
        "median": 0.8607333898544312,
        "min": 0.8165708780288696,
        "max": 0.889898955821991,
        "count": 13
      },
      "meteor": {
        "mean": 0.07206036634267725,
        "std": 0.09668777557828552,
        "median": 0.0,
        "min": 0.0,
        "max": 0.30000000000000004,
        "count": 13
      },
      "sbert_similarity": {
        "mean": 0.37174248981934327,
        "std": 0.17229594182053218,
        "median": 0.3988339304924011,
        "min": 0.05388104170560837,
        "max": 0.6093047857284546,
        "count": 13
      }
    },
    "category_4": {
      "exact_match": {
        "mean": 0.02857142857142857,
        "std": 0.16780151935092938,
        "median": 0.0,
        "min": 0,
        "max": 1,
        "count": 70
      },
      "f1": {
        "mean": 0.4129348613352903,
        "std": 0.27583415195790323,
        "median": 0.3851851851851852,
        "min": 0.0,
        "max": 1.0,
        "count": 70
      },
      "rouge1_f": {
        "mean": 0.43855655987250064,
        "std": 0.2819729558008093,
        "median": 0.4,
        "min": 0.0,
        "max": 1.0,
        "count": 70
      },
      "rouge2_f": {
        "mean": 0.3048781721220384,
        "std": 0.3063522742889533,
        "median": 0.2376470588235294,
        "min": 0.0,
        "max": 1.0,
        "count": 70
      },
      "rougeL_f": {
        "mean": 0.4264184504818743,
        "std": 0.2852713430718281,
        "median": 0.3923076923076923,
        "min": 0.0,
        "max": 1.0,
        "count": 70
      },
      "bleu1": {
        "mean": 0.3270349656962062,
        "std": 0.254965145889388,
        "median": 0.24264705882352944,
        "min": 0,
        "max": 1.0,
        "count": 70
      },
      "bleu2": {
        "mean": 0.2579873851963492,
        "std": 0.2484060874321785,
        "median": 0.1770363854887821,
        "min": 0,
        "max": 1.0,
        "count": 70
      },
      "bleu3": {
        "mean": 0.21029827517322314,
        "std": 0.23040508393961256,
        "median": 0.12111838948674812,
        "min": 0,
        "max": 0.8078434700407627,
        "count": 70
      },
      "bleu4": {
        "mean": 0.16940173181414278,
        "std": 0.2154409588766811,
        "median": 0.08161542093166782,
        "min": 0,
        "max": 0.7916963878457504,
        "count": 70
      },
      "bert_precision": {
        "mean": 0.8842331298760006,
        "std": 0.04711230994412953,
        "median": 0.8811192810535431,
        "min": 0.7893295288085938,
        "max": 1.0,
        "count": 70
      },
      "bert_recall": {
        "mean": 0.909648209810257,
        "std": 0.045743783165932514,
        "median": 0.911224365234375,
        "min": 0.8171942830085754,
        "max": 1.0,
        "count": 70
      },
      "bert_f1": {
        "mean": 0.8964836401598794,
        "std": 0.043736380706198404,
        "median": 0.8955117166042328,
        "min": 0.8030202388763428,
        "max": 1.0,
        "count": 70
      },
      "meteor": {
        "mean": 0.4194785218960686,
        "std": 0.3133998720160097,
        "median": 0.40241113915866833,
        "min": 0.0,
        "max": 0.9844782983615982,
        "count": 70
      },
      "sbert_similarity": {
        "mean": 0.5838399118078607,
        "std": 0.23870677647265634,
        "median": 0.6078897416591644,
        "min": 0.024385549128055573,
        "max": 1.0,
        "count": 70
      }
    },
    "category_5": {
      "exact_match": {
        "mean": 0.6170212765957447,
        "std": 0.491368607293056,
        "median": 1,
        "min": 0,
        "max": 1,
        "count": 47
      },
      "f1": {
        "mean": 0.6170212765957447,
        "std": 0.491368607293056,
        "median": 1.0,
        "min": 0.0,
        "max": 1.0,
        "count": 47
      },
      "rouge1_f": {
        "mean": 0.6170212765957447,
        "std": 0.491368607293056,
        "median": 1.0,
        "min": 0.0,
        "max": 1.0,
        "count": 47
      },
      "rouge2_f": {
        "mean": 0.5957446808510638,
        "std": 0.49605287534106185,
        "median": 1.0,
        "min": 0.0,
        "max": 1.0,
        "count": 47
      },
      "rougeL_f": {
        "mean": 0.6170212765957447,
        "std": 0.491368607293056,
        "median": 1.0,
        "min": 0.0,
        "max": 1.0,
        "count": 47
      },
      "bleu1": {
        "mean": 0.6170212765957447,
        "std": 0.491368607293056,
        "median": 1.0,
        "min": 0,
        "max": 1.0,
        "count": 47
      },
      "bleu2": {
        "mean": 0.6024729311918476,
        "std": 0.4899032570316895,
        "median": 1.0,
        "min": 0,
        "max": 1.0,
        "count": 47
      },
      "bleu3": {
        "mean": 0.57774992436105,
        "std": 0.4844775649752462,
        "median": 1.0,
        "min": 0,
        "max": 1.0,
        "count": 47
      },
      "bleu4": {
        "mean": 0.5145602430676524,
        "std": 0.45884291895479873,
        "median": 0.5623413251903491,
        "min": 0,
        "max": 1.0,
        "count": 47
      },
      "bert_precision": {
        "mean": 0.9489734210866563,
        "std": 0.06604875055544435,
        "median": 0.9999998807907104,
        "min": 0.8458929657936096,
        "max": 1.000000238418579,
        "count": 47
      },
      "bert_recall": {
        "mean": 0.9395498544611829,
        "std": 0.08177639336138759,
        "median": 0.9999998807907104,
        "min": 0.734740138053894,
        "max": 1.000000238418579,
        "count": 47
      },
      "bert_f1": {
        "mean": 0.9439310431480408,
        "std": 0.07308076517281308,
        "median": 0.9999998807907104,
        "min": 0.7939960956573486,
        "max": 1.000000238418579,
        "count": 47
      },
      "meteor": {
        "mean": 0.6002107638404423,
        "std": 0.4833821651331371,
        "median": 0.9814814814814815,
        "min": 0.0,
        "max": 0.9998177842565598,
        "count": 47
      },
      "sbert_similarity": {
        "mean": 0.6464598315391452,
        "std": 0.45525486097963963,
        "median": 1.0,
        "min": -0.0609419085085392,
        "max": 1.000000238418579,
        "count": 47
      }
    }
  },
  "individual_results": [
    {
      "sample_id": 0,
      "question": "When did Caroline go to the LGBTQ support group?",
      "prediction": "8 May, 2023",
      "reference": "7 May 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.5,
        "bleu2": 0.12909944487358058,
        "bleu3": 0.09635409769034631,
        "bleu4": 0.09554427922043669,
        "bert_precision": 0.9594283103942871,
        "bert_recall": 0.9838893413543701,
        "bert_f1": 0.9715048670768738,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.8955166339874268
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "When did Melanie paint a sunrise?",
      "prediction": "last year",
      "reference": "2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8694185614585876,
        "bert_recall": 0.9111440777778625,
        "bert_f1": 0.8897924423217773,
        "meteor": 0.0,
        "sbert_similarity": 0.3672371506690979
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What fields would Caroline be likely to pursue in her educaton?",
      "prediction": "I'm keen on counseling or working in mental health - I'd love to support those with similar issues.",
      "reference": "Psychology, counseling certification",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.09523809523809525,
        "rouge1_f": 0.09090909090909091,
        "rouge2_f": 0.0,
        "rougeL_f": 0.09090909090909091,
        "bleu1": 0.047619047619047616,
        "bleu2": 0.01543033499620919,
        "bleu3": 0.011280777824436245,
        "bleu4": 0.00913442366656447,
        "bert_precision": 0.823591411113739,
        "bert_recall": 0.8548350930213928,
        "bert_f1": 0.8389225006103516,
        "meteor": 0.11111111111111113,
        "sbert_similarity": 0.4839847981929779
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What did Caroline research?",
      "prediction": "researching adoption agencies",
      "reference": "Adoption agencies",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8,
        "rouge1_f": 0.8,
        "rouge2_f": 0.6666666666666666,
        "rougeL_f": 0.8,
        "bleu1": 0.6666666666666666,
        "bleu2": 0.5773502691896257,
        "bleu3": 0.3254992452916931,
        "bleu4": 0.24028114141347542,
        "bert_precision": 0.9506014585494995,
        "bert_recall": 0.9506014585494995,
        "bert_f1": 0.9506014585494995,
        "meteor": 0.8928571428571429,
        "sbert_similarity": 0.8755380511283875
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What is Caroline's identity?",
      "prediction": "Caroline is a transgender woman.",
      "reference": "Transgender woman",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5714285714285715,
        "rouge1_f": 0.5714285714285715,
        "rouge2_f": 0.4,
        "rougeL_f": 0.5714285714285715,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.25819888974716115,
        "bleu3": 0.12111838948674812,
        "bleu4": 0.08633400213704504,
        "bert_precision": 0.8655157089233398,
        "bert_recall": 0.8890457153320312,
        "bert_f1": 0.8771229386329651,
        "meteor": 0.21739130434782608,
        "sbert_similarity": 0.6492607593536377
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "When did Melanie run a charity race?",
      "prediction": "last Saturday, 20 May 2023",
      "reference": "The sunday before 25 May 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3636363636363636,
        "rouge1_f": 0.3636363636363636,
        "rouge2_f": 0.22222222222222224,
        "rougeL_f": 0.3636363636363636,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.25819888974716115,
        "bleu3": 0.12111838948674812,
        "bleu4": 0.08633400213704504,
        "bert_precision": 0.9251143932342529,
        "bert_recall": 0.9049964547157288,
        "bert_f1": 0.9149448871612549,
        "meteor": 0.3177966101694915,
        "sbert_similarity": 0.8413609862327576
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "When is Melanie planning on going camping?",
      "prediction": "next month",
      "reference": "June 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8738219738006592,
        "bert_recall": 0.8416208624839783,
        "bert_f1": 0.8574191331863403,
        "meteor": 0.0,
        "sbert_similarity": 0.567993700504303
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What is Caroline's relationship status?",
      "prediction": "Caroline is a single parent.",
      "reference": "Single",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.33333333333333337,
        "rouge1_f": 0.33333333333333337,
        "rouge2_f": 0.0,
        "rougeL_f": 0.33333333333333337,
        "bleu1": 0.16666666666666669,
        "bleu2": 0.05773502691896258,
        "bleu3": 0.04506819749679463,
        "bleu4": 0.040824829046386304,
        "bert_precision": 0.8418756723403931,
        "bert_recall": 0.8707572221755981,
        "bert_f1": 0.8560729026794434,
        "meteor": 0.35714285714285715,
        "sbert_similarity": 0.3093712031841278
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "When did Caroline give a speech at a school?",
      "prediction": "9 June 2023",
      "reference": "The week before 9 June 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5714285714285715,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.36787944117144233,
        "bleu2": 0.36787944117144233,
        "bleu3": 0.36787944117144233,
        "bleu4": 0.20687381245863395,
        "bert_precision": 0.9531698822975159,
        "bert_recall": 0.8788211345672607,
        "bert_f1": 0.9144868850708008,
        "meteor": 0.5165692007797271,
        "sbert_similarity": 0.8633456230163574
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "When did Caroline meet up with her friends, family, and mentors?",
      "prediction": "last week",
      "reference": "The week before 9 June 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.06766764161830635,
        "bleu2": 0.030261889307125377,
        "bleu3": 0.02355435073584915,
        "bleu4": 0.020237345084409754,
        "bert_precision": 0.8893059492111206,
        "bert_recall": 0.8029053211212158,
        "bert_f1": 0.8438999056816101,
        "meteor": 0.08928571428571427,
        "sbert_similarity": 0.5041569471359253
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "How long has Caroline had her current group of friends for?",
      "prediction": "known these friends for 4 years",
      "reference": "4 years",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.33333333333333337,
        "rougeL_f": 0.5,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.25819888974716115,
        "bleu3": 0.12111838948674812,
        "bleu4": 0.08633400213704504,
        "bert_precision": 0.8538206815719604,
        "bert_recall": 0.8984984159469604,
        "bert_f1": 0.8755900263786316,
        "meteor": 0.78125,
        "sbert_similarity": 0.4216121435165405
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "Where did Caroline move from 4 years ago?",
      "prediction": "my home country",
      "reference": "Sweden",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.814621090888977,
        "bert_recall": 0.8900223970413208,
        "bert_f1": 0.850654125213623,
        "meteor": 0.0,
        "sbert_similarity": 0.48347207903862
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "How long ago was Caroline's 18th birthday?",
      "prediction": "10 years ago",
      "reference": "10 years ago",
      "category": 2,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What career path has Caroline decided to persue?",
      "prediction": "I'm thinking of working with trans people, helping them accept themselves and supporting their mental health.",
      "reference": "counseling or mental health for Transgender people",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.26086956521739124,
        "rouge1_f": 0.25,
        "rouge2_f": 0.09090909090909091,
        "rougeL_f": 0.16666666666666666,
        "bleu1": 0.15789473684210523,
        "bleu2": 0.09365858115816939,
        "bleu3": 0.03847452395409486,
        "bleu4": 0.023830412565256146,
        "bert_precision": 0.8658164739608765,
        "bert_recall": 0.8829202651977539,
        "bert_f1": 0.8742847442626953,
        "meteor": 0.06329113924050632,
        "sbert_similarity": 0.6562923192977905
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "Would Caroline still want to pursue counseling as a career if she hadn't received support growing up?",
      "prediction": "I want to pass that same support to anyone who needs it.",
      "reference": "Likely no",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8139374256134033,
        "bert_recall": 0.8192214965820312,
        "bert_f1": 0.8165708780288696,
        "meteor": 0.16666666666666669,
        "sbert_similarity": 0.12115907669067383
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What activities does Melanie partake in?",
      "prediction": "I love painting together lately, especially nature-inspired ones.",
      "reference": "pottery, camping, painting, swimming",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.16666666666666666,
        "rouge1_f": 0.15384615384615383,
        "rouge2_f": 0.0,
        "rougeL_f": 0.15384615384615383,
        "bleu1": 0.2,
        "bleu2": 0.04714045207910317,
        "bleu3": 0.03136318607896979,
        "bleu4": 0.025098621243978974,
        "bert_precision": 0.8585062623023987,
        "bert_recall": 0.8474375605583191,
        "bert_f1": 0.8529360294342041,
        "meteor": 0.0,
        "sbert_similarity": 0.4035677909851074
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "When did Melanie sign up for a pottery class?",
      "prediction": "27 June, 2023",
      "reference": "2 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.25,
        "bleu2": 0.09128709291752769,
        "bleu3": 0.07665320007195604,
        "bleu4": 0.08034284189446518,
        "bert_precision": 0.9456434845924377,
        "bert_recall": 0.9671379327774048,
        "bert_f1": 0.9562699198722839,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.8851857781410217
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "When is Caroline going to the transgender conference?",
      "prediction": "3 July, 2023",
      "reference": "July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8,
        "rouge1_f": 0.8,
        "rouge2_f": 0.6666666666666666,
        "rougeL_f": 0.8,
        "bleu1": 0.5,
        "bleu2": 0.12909944487358058,
        "bleu3": 0.09635409769034631,
        "bleu4": 0.09554427922043669,
        "bert_precision": 0.877350926399231,
        "bert_recall": 0.9392186999320984,
        "bert_f1": 0.9072312712669373,
        "meteor": 0.2380952380952381,
        "sbert_similarity": 0.9297047853469849
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "Where has Melanie camped?",
      "prediction": "camping at the beach",
      "reference": "beach, mountains, forest",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.28571428571428575,
        "rouge1_f": 0.28571428571428575,
        "rouge2_f": 0.0,
        "rougeL_f": 0.28571428571428575,
        "bleu1": 0.19470019576785122,
        "bleu2": 0.07109445944848267,
        "bleu3": 0.05969757224096844,
        "bleu4": 0.06257106818159155,
        "bert_precision": 0.8526665568351746,
        "bert_recall": 0.8064661622047424,
        "bert_f1": 0.8289231061935425,
        "meteor": 0.0,
        "sbert_similarity": 0.5625999569892883
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What do Melanie's kids like?",
      "prediction": "the kids love learning about animals and the bones were so cool.",
      "reference": "dinosaurs, nature",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8207933306694031,
        "bert_recall": 0.8085970282554626,
        "bert_f1": 0.8146495223045349,
        "meteor": 0.0,
        "sbert_similarity": 0.4139253795146942
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "When did Melanie go to the museum?",
      "prediction": "1:14 pm on 25 May, 2023",
      "reference": "5 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2222222222222222,
        "rouge1_f": 0.2,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2,
        "bleu1": 0.14285714285714285,
        "bleu2": 0.048795003647426664,
        "bleu3": 0.037468614372419724,
        "bleu4": 0.033031643180138064,
        "bert_precision": 0.8388646841049194,
        "bert_recall": 0.8987348675727844,
        "bert_f1": 0.8677682876586914,
        "meteor": 0.15151515151515152,
        "sbert_similarity": 0.7314975261688232
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "When did Caroline have a picnic?",
      "prediction": "last week",
      "reference": "The week before 6 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.06766764161830635,
        "bleu2": 0.030261889307125377,
        "bleu3": 0.02355435073584915,
        "bleu4": 0.020237345084409754,
        "bert_precision": 0.8984266519546509,
        "bert_recall": 0.8077255487442017,
        "bert_f1": 0.8506652116775513,
        "meteor": 0.08928571428571427,
        "sbert_similarity": 0.49372273683547974
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "Would Caroline likely have Dr. Seuss books on her bookshelf?",
      "prediction": "I've got lots of kids' books- classics, stories from different cultures, educational books, all of that.",
      "reference": "Yes, since she collects classic children's books",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.09090909090909091,
        "rouge1_f": 0.15999999999999998,
        "rouge2_f": 0.0,
        "rougeL_f": 0.15999999999999998,
        "bleu1": 0.0909090909090909,
        "bleu2": 0.020806259464411975,
        "bleu3": 0.013510398157576722,
        "bleu4": 0.010331208012220438,
        "bert_precision": 0.8653685450553894,
        "bert_recall": 0.8898049592971802,
        "bert_f1": 0.8774166703224182,
        "meteor": 0.0,
        "sbert_similarity": 0.5071793794631958
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What books has Melanie read?",
      "prediction": "the book you recommended a while ago",
      "reference": "\"Nothing is Impossible\", \"Charlotte's Web\"",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8481577634811401,
        "bert_recall": 0.8231124877929688,
        "bert_f1": 0.8354474306106567,
        "meteor": 0.0,
        "sbert_similarity": 0.24180561304092407
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What does Melanie do to destress?",
      "prediction": "I've been running farther to de-stress, which has been great for my headspace.",
      "reference": "Running, pottery",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.14285714285714285,
        "rouge1_f": 0.11764705882352941,
        "rouge2_f": 0.0,
        "rougeL_f": 0.11764705882352941,
        "bleu1": 0.12500000000000003,
        "bleu2": 0.028867513459481287,
        "bleu3": 0.018864614597621485,
        "bleu4": 0.014628063653657535,
        "bert_precision": 0.8261620998382568,
        "bert_recall": 0.8199144005775452,
        "bert_f1": 0.8230264782905579,
        "meteor": 0.0,
        "sbert_similarity": 0.23419293761253357
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "When did Caroline go to the LGBTQ conference?",
      "prediction": "3 July, 2023",
      "reference": "10 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.5,
        "bleu2": 0.12909944487358058,
        "bleu3": 0.09635409769034631,
        "bleu4": 0.09554427922043669,
        "bert_precision": 0.9586054086685181,
        "bert_recall": 0.9833974242210388,
        "bert_f1": 0.9708431959152222,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.8341851234436035
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "When did Melanie read the book \"nothing is impossible\"?",
      "prediction": "October 13, 2023",
      "reference": "2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7658332586288452,
        "bert_recall": 0.8590919971466064,
        "bert_f1": 0.8097864389419556,
        "meteor": 0.0,
        "sbert_similarity": 0.6109983921051025
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "Would Caroline pursue writing as a career option?",
      "prediction": "I'm still figuring out the details, but I'm thinking of working with trans people, helping them accept themselves and supporting their mental health.",
      "reference": "LIkely no; though she likes reading, she wants to be a counselor",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0.03571428571428572,
        "bleu2": 0.011501092655705905,
        "bleu3": 0.008378120504949009,
        "bleu4": 0.006716472052354862,
        "bert_precision": 0.8498144745826721,
        "bert_recall": 0.8459872603416443,
        "bert_f1": 0.8478965163230896,
        "meteor": 0.0,
        "sbert_similarity": 0.25399303436279297
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "When did Caroline go to the adoption meeting?",
      "prediction": "15 July, 2023",
      "reference": "The friday before 15 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5714285714285715,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.45489799478447507,
        "bleu2": 0.30326532985631666,
        "bleu3": 0.14283448174223617,
        "bleu4": 0.11404605374835301,
        "bert_precision": 0.9278004765510559,
        "bert_recall": 0.8661315441131592,
        "bert_f1": 0.8959060907363892,
        "meteor": 0.17543859649122806,
        "sbert_similarity": 0.7955406308174133
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "When did Melanie go to the pottery workshop?",
      "prediction": "15 July, 2023",
      "reference": "The Friday before 15 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5714285714285715,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.45489799478447507,
        "bleu2": 0.30326532985631666,
        "bleu3": 0.14283448174223617,
        "bleu4": 0.11404605374835301,
        "bert_precision": 0.9329335689544678,
        "bert_recall": 0.886917233467102,
        "bert_f1": 0.9093436002731323,
        "meteor": 0.17543859649122806,
        "sbert_similarity": 0.7955406308174133
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "Would Melanie be considered a member of the LGBTQ community?",
      "prediction": "No, the context does not indicate that Melanie identifies as a member of the LGBTQ community.",
      "reference": "Likely no, she does not refer to herself as part of it",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3703703703703704,
        "rouge1_f": 0.35714285714285715,
        "rouge2_f": 0.07692307692307691,
        "rougeL_f": 0.35714285714285715,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.19802950859533483,
        "bleu3": 0.06434016142474894,
        "bleu4": 0.03575297164449809,
        "bert_precision": 0.8920342922210693,
        "bert_recall": 0.8877737522125244,
        "bert_f1": 0.889898955821991,
        "meteor": 0.30000000000000004,
        "sbert_similarity": 0.6093047857284546
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "When did Melanie go camping in June?",
      "prediction": "6 July 2023",
      "reference": "The week before 27 June 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2222222222222222,
        "rouge1_f": 0.2222222222222222,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2222222222222222,
        "bleu1": 0.12262648039048077,
        "bleu2": 0.04749303163563625,
        "bleu3": 0.044556965439969995,
        "bleu4": 0.0417991745584234,
        "bert_precision": 0.9447920918464661,
        "bert_recall": 0.8735885620117188,
        "bert_f1": 0.907796323299408,
        "meteor": 0.08771929824561403,
        "sbert_similarity": 0.7713804244995117
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What LGBTQ+ events has Caroline participated in?",
      "prediction": "I went to an LGBTQ support group yesterday and it was so powerful. I went to an LGBTQ+ pride parade. Last weekend our city held a pride parade! I just joined a new LGBTQ activist group last Tues. I went to a transgender poetry reading where transgender people shared their stories through poetry.",
      "reference": "Pride parade, school speech, support group",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.1904761904761905,
        "rouge1_f": 0.13559322033898305,
        "rouge2_f": 0.07017543859649124,
        "rougeL_f": 0.1016949152542373,
        "bleu1": 0.07017543859649124,
        "bleu2": 0.0500626174321759,
        "bleu3": 0.017272661295035737,
        "bleu4": 0.009584460320418755,
        "bert_precision": 0.8292524814605713,
        "bert_recall": 0.8571916818618774,
        "bert_f1": 0.8429906368255615,
        "meteor": 0.14018691588785048,
        "sbert_similarity": 0.5692493915557861
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "When did Caroline go to a pride parade during the summer?",
      "prediction": "3 July, 2023",
      "reference": "The week before 3 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5714285714285715,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.45489799478447507,
        "bleu2": 0.30326532985631666,
        "bleu3": 0.14283448174223617,
        "bleu4": 0.11404605374835301,
        "bert_precision": 0.9270618557929993,
        "bert_recall": 0.8871561288833618,
        "bert_f1": 0.906670093536377,
        "meteor": 0.17543859649122806,
        "sbert_similarity": 0.8097395896911621
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What events has Caroline participated in to help children?",
      "prediction": "I went to an LGBTQ+ youth center. It was so gratifying to talk to similar young people.",
      "reference": "Mentoring program, school speech",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8365330696105957,
        "bert_recall": 0.8265941739082336,
        "bert_f1": 0.8315339684486389,
        "meteor": 0.0,
        "sbert_similarity": 0.310067743062973
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "When did Melanie go camping in July?",
      "prediction": "5 July, 2023",
      "reference": "two weekends before 17 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4444444444444444,
        "rouge1_f": 0.4444444444444444,
        "rouge2_f": 0.28571428571428575,
        "rougeL_f": 0.4444444444444444,
        "bleu1": 0.3032653298563167,
        "bleu2": 0.07830277146770757,
        "bleu3": 0.05844171443814127,
        "bleu4": 0.05795053470733952,
        "bert_precision": 0.9185322523117065,
        "bert_recall": 0.880333662033081,
        "bert_f1": 0.8990273475646973,
        "meteor": 0.08771929824561403,
        "sbert_similarity": 0.695436954498291
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "When did Caroline join a mentorship program?",
      "prediction": "17 July 2023",
      "reference": "The weekend before 17 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5714285714285715,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.36787944117144233,
        "bleu2": 0.36787944117144233,
        "bleu3": 0.36787944117144233,
        "bleu4": 0.20687381245863395,
        "bert_precision": 0.9512652158737183,
        "bert_recall": 0.8775990605354309,
        "bert_f1": 0.912948489189148,
        "meteor": 0.5165692007797271,
        "sbert_similarity": 0.7773944139480591
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What did Melanie paint recently?",
      "prediction": "Here's one I did last week. It's inspired by the sunsets. The colors make me feel calm.",
      "reference": "sunset",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.1,
        "rouge2_f": 0.0,
        "rougeL_f": 0.1,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8155382871627808,
        "bert_recall": 0.8079465627670288,
        "bert_f1": 0.8117246627807617,
        "meteor": 0.0,
        "sbert_similarity": 0.47694775462150574
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What activities has Melanie done with her family?",
      "prediction": "took my kids to a pottery workshop, went camping with the kids, explored nature, roasted marshmallows around the campfire, saw the Perseid meteor shower, and spent the day with my fam volunteering at a homeless shelter.",
      "reference": "Pottery, painting, camping, museum, swimming, hiking",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.1142857142857143,
        "rouge1_f": 0.09523809523809525,
        "rouge2_f": 0.0,
        "rougeL_f": 0.09523809523809525,
        "bleu1": 0.16666666666666669,
        "bleu2": 0.0201619459636378,
        "bleu3": 0.010527169545243902,
        "bleu4": 0.007144717698130144,
        "bert_precision": 0.8151048421859741,
        "bert_recall": 0.8474459648132324,
        "bert_f1": 0.8309608697891235,
        "meteor": 0.0,
        "sbert_similarity": 0.559495210647583
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "In what ways is Caroline participating in the LGBTQ community?",
      "prediction": "I went to a LGBTQ support group yesterday and it was so powerful.",
      "reference": "Joining activist group, going to pride parades, participating in an art show, mentoring program",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.14814814814814817,
        "rouge1_f": 0.14814814814814817,
        "rouge2_f": 0.0,
        "rougeL_f": 0.07407407407407408,
        "bleu1": 0.1153025352864842,
        "bleu2": 0.026755692780938665,
        "bleu3": 0.017551871312838876,
        "bleu4": 0.0137098614488229,
        "bert_precision": 0.8686299324035645,
        "bert_recall": 0.8392164707183838,
        "bert_f1": 0.8536699414253235,
        "meteor": 0.13489208633093525,
        "sbert_similarity": 0.46658170223236084
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "How many times has Melanie gone to the beach in 2023?",
      "prediction": "usually only once or twice a year",
      "reference": "2",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.783317506313324,
        "bert_recall": 0.8847261667251587,
        "bert_f1": 0.8309392929077148,
        "meteor": 0.0,
        "sbert_similarity": 0.17895720899105072
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "When did Caroline join a new activist group?",
      "prediction": "20 July 2023",
      "reference": "The Tuesday before 20 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5714285714285715,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.36787944117144233,
        "bleu2": 0.36787944117144233,
        "bleu3": 0.36787944117144233,
        "bleu4": 0.20687381245863395,
        "bert_precision": 0.9568144083023071,
        "bert_recall": 0.8813892602920532,
        "bert_f1": 0.9175544381141663,
        "meteor": 0.5165692007797271,
        "sbert_similarity": 0.8518599271774292
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "Would Melanie be more interested in going to a national park or a theme park?",
      "prediction": "I love camping trips with my fam, 'cause nature brings such peace and serenity.",
      "reference": "National park; she likes the outdoors",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8502247333526611,
        "bert_recall": 0.8911182880401611,
        "bert_f1": 0.8701913356781006,
        "meteor": 0.0,
        "sbert_similarity": 0.3988339304924011
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What kind of art does Caroline make?",
      "prediction": "I love creating art!",
      "reference": "abstract art",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.2,
        "bleu2": 0.07071067811865477,
        "bleu3": 0.05665132701906204,
        "bleu4": 0.05372849659117709,
        "bert_precision": 0.8636104464530945,
        "bert_recall": 0.9052507877349854,
        "bert_f1": 0.8839404582977295,
        "meteor": 0.0,
        "sbert_similarity": 0.5159320831298828
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "When is Melanie's daughter's birthday?",
      "prediction": "14 August 2023",
      "reference": "13 August",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.12909944487358058,
        "bleu3": 0.12111838948674812,
        "bleu4": 0.11362193664674995,
        "bert_precision": 0.8807874917984009,
        "bert_recall": 0.9523084759712219,
        "bert_f1": 0.9151527285575867,
        "meteor": 0.2380952380952381,
        "sbert_similarity": 0.7068345546722412
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "When did Caroline attend a pride parade in August?",
      "prediction": "last Friday in August",
      "reference": "The Friday before 14 August 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.3032653298563167,
        "bleu2": 0.07830277146770757,
        "bleu3": 0.05844171443814127,
        "bleu4": 0.05795053470733952,
        "bert_precision": 0.8957915306091309,
        "bert_recall": 0.8455819487571716,
        "bert_f1": 0.8699628114700317,
        "meteor": 0.17241379310344826,
        "sbert_similarity": 0.6210903525352478
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "Would Melanie be considered an ally to the transgender community?",
      "prediction": "You're so inspiring for wanting to help others with their mental health.",
      "reference": "Yes, she is supportive",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8426485657691956,
        "bert_recall": 0.8560183048248291,
        "bert_f1": 0.849280834197998,
        "meteor": 0.0,
        "sbert_similarity": 0.3060126304626465
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "Who supports Caroline when she has a negative experience?",
      "prediction": "I know I'll be ok!",
      "reference": "Her mentors, family, and friends",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8234542608261108,
        "bert_recall": 0.8371400833129883,
        "bert_f1": 0.8302407264709473,
        "meteor": 0.0,
        "sbert_similarity": 0.06914487481117249
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What types of pottery have Melanie and her kids made?",
      "prediction": "made our own pots, it was fun and therapeutic!",
      "reference": "bowls, cup",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0.0909090909090909,
        "bleu2": 0.03015113445777637,
        "bleu3": 0.022461584318318094,
        "bleu4": 0.018850319022747353,
        "bert_precision": 0.8201529383659363,
        "bert_recall": 0.8376978635787964,
        "bert_f1": 0.8288326263427734,
        "meteor": 0.0,
        "sbert_similarity": 0.2547297477722168
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "When did Caroline and Melanie go to a pride fesetival together?",
      "prediction": "last year",
      "reference": "2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8694185614585876,
        "bert_recall": 0.9111440777778625,
        "bert_f1": 0.8897924423217773,
        "meteor": 0.0,
        "sbert_similarity": 0.3672371506690979
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What would Caroline's political leaning likely be?",
      "prediction": "fighting for LGBTQ rights and spread awareness",
      "reference": "Liberal",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7937987446784973,
        "bert_recall": 0.8436731100082397,
        "bert_f1": 0.8179763555526733,
        "meteor": 0.0,
        "sbert_similarity": 0.337567538022995
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What has Melanie painted?",
      "prediction": "a painting of a sunset with a pink sky",
      "reference": "Horse, sunset, sunrise",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2,
        "rouge1_f": 0.16666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.16666666666666666,
        "bleu1": 0.11111111111111109,
        "bleu2": 0.03726779962499649,
        "bleu3": 0.028067124621919234,
        "bleu4": 0.023980296761827107,
        "bert_precision": 0.8540064096450806,
        "bert_recall": 0.8754277229309082,
        "bert_f1": 0.8645843863487244,
        "meteor": 0.0,
        "sbert_similarity": 0.5285056829452515
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What are Melanie's pets' names?",
      "prediction": "Luna and Oliver",
      "reference": "Oliver, Luna, Bailey",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.3422780793550613,
        "bleu2": 0.09373671250115519,
        "bleu3": 0.07816643970700751,
        "bleu4": 0.069372929071742,
        "bert_precision": 0.907483696937561,
        "bert_recall": 0.8730454444885254,
        "bert_f1": 0.8899315595626831,
        "meteor": 0.0,
        "sbert_similarity": 0.8309195041656494
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "When did Caroline apply to adoption agencies?",
      "prediction": "23 August 2023",
      "reference": "The week of 23 August 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5714285714285715,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.36787944117144233,
        "bleu2": 0.36787944117144233,
        "bleu3": 0.36787944117144233,
        "bleu4": 0.20687381245863395,
        "bert_precision": 0.9526714086532593,
        "bert_recall": 0.8778687715530396,
        "bert_f1": 0.9137417078018188,
        "meteor": 0.5165692007797271,
        "sbert_similarity": 0.8445632457733154
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "When did Caroline draw a self-portrait?",
      "prediction": "last week",
      "reference": "The week before 23 August 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.06766764161830635,
        "bleu2": 0.030261889307125377,
        "bleu3": 0.02355435073584915,
        "bleu4": 0.020237345084409754,
        "bert_precision": 0.8945940732955933,
        "bert_recall": 0.8071079850196838,
        "bert_f1": 0.8486021757125854,
        "meteor": 0.08928571428571427,
        "sbert_similarity": 0.4893570840358734
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What subject have Caroline and Melanie both painted?",
      "prediction": "nature",
      "reference": "Sunsets",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8955835700035095,
        "bert_recall": 0.8155258893966675,
        "bert_f1": 0.8536819219589233,
        "meteor": 0.0,
        "sbert_similarity": 0.35048747062683105
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What symbols are important to Caroline?",
      "prediction": "The rainbow flag mural is important to me as it reflects the courage and strength of the trans community. The eagle symbolizes freedom and pride, representing my own resilience and that of others.",
      "reference": "Rainbow flag, transgender symbol",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.12903225806451613,
        "rouge1_f": 0.16216216216216214,
        "rouge2_f": 0.05714285714285714,
        "rougeL_f": 0.16216216216216214,
        "bleu1": 0.08333333333333333,
        "bleu2": 0.048795003647426664,
        "bleu3": 0.019903969226299174,
        "bleu4": 0.012069505116436091,
        "bert_precision": 0.8333467245101929,
        "bert_recall": 0.8834269642829895,
        "bert_f1": 0.8576564192771912,
        "meteor": 0.14492753623188406,
        "sbert_similarity": 0.623482346534729
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "When did Caroline encounter people on a hike and have a negative experience?",
      "prediction": "25 August 2023",
      "reference": "The week before 25 August 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5714285714285715,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.36787944117144233,
        "bleu2": 0.36787944117144233,
        "bleu3": 0.36787944117144233,
        "bleu4": 0.20687381245863395,
        "bert_precision": 0.9580475091934204,
        "bert_recall": 0.8832759857177734,
        "bert_f1": 0.9191436171531677,
        "meteor": 0.5165692007797271,
        "sbert_similarity": 0.838809609413147
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "When did Melanie make a plate in pottery class?",
      "prediction": "17 August, 2023",
      "reference": "24 August 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.5,
        "bleu2": 0.12909944487358058,
        "bleu3": 0.09635409769034631,
        "bleu4": 0.09554427922043669,
        "bert_precision": 0.9623751640319824,
        "bert_recall": 0.9868565797805786,
        "bert_f1": 0.9744621515274048,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.9297324419021606
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "Would Caroline be considered religious?",
      "prediction": "Caroline does not express any religious beliefs in the context.",
      "reference": "Somewhat, but not extremely religious",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.26666666666666666,
        "rouge1_f": 0.26666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.26666666666666666,
        "bleu1": 0.18181818181818182,
        "bleu2": 0.042640143271122095,
        "bleu3": 0.028234511901075607,
        "bleu4": 0.022416933501922302,
        "bert_precision": 0.87245774269104,
        "bert_recall": 0.8493199348449707,
        "bert_f1": 0.8607333898544312,
        "meteor": 0.18181818181818182,
        "sbert_similarity": 0.432700514793396
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What instruments does Melanie play?",
      "prediction": "I play clarinet!",
      "reference": "clarinet and violin",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.25,
        "bleu2": 0.09128709291752769,
        "bleu3": 0.07665320007195604,
        "bleu4": 0.08034284189446518,
        "bert_precision": 0.8622114658355713,
        "bert_recall": 0.8664951324462891,
        "bert_f1": 0.8643479943275452,
        "meteor": 0.0,
        "sbert_similarity": 0.7264347076416016
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What musical artists/bands has Melanie seen?",
      "prediction": "Matt Patterson",
      "reference": "Summer Sounds, Matt Patterson",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.22313016014842982,
        "bleu2": 0.22313016014842982,
        "bleu3": 0.10436581698246099,
        "bleu4": 0.07055995207471726,
        "bert_precision": 0.9318119287490845,
        "bert_recall": 0.8726571798324585,
        "bert_f1": 0.9012649655342102,
        "meteor": 0.4934210526315789,
        "sbert_similarity": 0.6367626190185547
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "When did Melanie go to the park?",
      "prediction": "20 October 2023",
      "reference": "27 August 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.12909944487358058,
        "bleu3": 0.12111838948674812,
        "bleu4": 0.11362193664674995,
        "bert_precision": 0.9826233983039856,
        "bert_recall": 0.9826233983039856,
        "bert_f1": 0.9826233983039856,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.8475873470306396
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "When is Caroline's youth center putting on a talent show?",
      "prediction": "next month",
      "reference": "September 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8689342737197876,
        "bert_recall": 0.8372640013694763,
        "bert_f1": 0.8528052568435669,
        "meteor": 0.0,
        "sbert_similarity": 0.536835789680481
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "Would Melanie likely enjoy the song \"The Four Seasons\" by Vivaldi?",
      "prediction": "I'm a fan of both classical like Bach and Mozart.",
      "reference": "Yes; it's classical music",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.14285714285714288,
        "rouge1_f": 0.12500000000000003,
        "rouge2_f": 0.0,
        "rougeL_f": 0.12500000000000003,
        "bleu1": 0.08333333333333333,
        "bleu2": 0.027524094128159027,
        "bleu3": 0.020427282860665447,
        "bleu4": 0.017033186037639283,
        "bert_precision": 0.8660361170768738,
        "bert_recall": 0.8776320219039917,
        "bert_f1": 0.8717955350875854,
        "meteor": 0.10869565217391304,
        "sbert_similarity": 0.5727803707122803
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What are some changes Caroline has faced during her transition journey?",
      "prediction": "I felt super powerful giving my talk. I shared my own journey, the struggles I had and how much I've developed since coming out.",
      "reference": "Changes to her body, losing unsupportive friends",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0.03571428571428572,
        "bleu2": 0.011501092655705905,
        "bleu3": 0.008378120504949009,
        "bleu4": 0.006716472052354862,
        "bert_precision": 0.847102165222168,
        "bert_recall": 0.8349159955978394,
        "bert_f1": 0.8409649133682251,
        "meteor": 0.0,
        "sbert_similarity": 0.19359225034713745
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What does Melanie do with her family on hikes?",
      "prediction": "We bond over stories, campfires and nature.",
      "reference": "Roast marshmallows, tell stories",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.18181818181818182,
        "rouge1_f": 0.18181818181818182,
        "rouge2_f": 0.0,
        "rougeL_f": 0.18181818181818182,
        "bleu1": 0.2222222222222222,
        "bleu2": 0.05270462766947299,
        "bleu3": 0.03528075103412339,
        "bleu4": 0.028517539529041507,
        "bert_precision": 0.8922949433326721,
        "bert_recall": 0.8588370084762573,
        "bert_f1": 0.8752462863922119,
        "meteor": 0.0,
        "sbert_similarity": 0.37100154161453247
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "When did Caroline go biking with friends?",
      "prediction": "weekend of 13 October 2023",
      "reference": "The weekend before 13 September 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5454545454545454,
        "rouge1_f": 0.5454545454545454,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5454545454545454,
        "bleu1": 0.49123845184678916,
        "bleu2": 0.10027362908828316,
        "bleu3": 0.06665016134041894,
        "bleu4": 0.05789300674674099,
        "bert_precision": 0.9298536777496338,
        "bert_recall": 0.8963835835456848,
        "bert_f1": 0.9128119349479675,
        "meteor": 0.25423728813559315,
        "sbert_similarity": 0.9141335487365723
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "How long has Melanie been practicing art?",
      "prediction": "seven years",
      "reference": "Since 2016",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9032423496246338,
        "bert_recall": 0.9032423496246338,
        "bert_f1": 0.9032423496246338,
        "meteor": 0.0,
        "sbert_similarity": 0.4254473149776459
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What personality traits might Melanie say Caroline has?",
      "prediction": "caring, strong, inspiring, brave",
      "reference": "Thoughtful, authentic, driven",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0.2857142857142857,
        "bleu2": 0.06900655593423542,
        "bleu3": 0.047098549390932394,
        "bleu4": 0.039281465090051315,
        "bert_precision": 0.8946107029914856,
        "bert_recall": 0.8821642994880676,
        "bert_f1": 0.8883439302444458,
        "meteor": 0.0,
        "sbert_similarity": 0.5203455686569214
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What transgender-specific events has Caroline attended?",
      "prediction": "I went to an LGBTQ support group yesterday and it was so powerful. Last week I went to an LGBTQ+ pride parade. I went to an LGBTQ+ counseling workshop and it was really enlightening. Last weekend our city held a pride parade! So many people marched through the streets waving flags, holding signs and celebrating love and diversity.",
      "reference": "Poetry reading, conference",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0.015625000000000007,
        "bleu2": 0.004980119205559976,
        "bleu3": 0.0036198366239847198,
        "bleu4": 0.00284570245153172,
        "bert_precision": 0.8171546459197998,
        "bert_recall": 0.8447449207305908,
        "bert_f1": 0.8307207822799683,
        "meteor": 0.0,
        "sbert_similarity": 0.16420665383338928
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What book did Melanie read from Caroline's suggestion?",
      "prediction": "that book you recommended a while ago",
      "reference": "\"Becoming Nicole\"",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8276762962341309,
        "bert_recall": 0.8288825750350952,
        "bert_f1": 0.8282790184020996,
        "meteor": 0.0,
        "sbert_similarity": 0.11817921698093414
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "When did Melanie's friend adopt a child?",
      "prediction": "last year",
      "reference": "2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8694185614585876,
        "bert_recall": 0.9111440777778625,
        "bert_f1": 0.8897924423217773,
        "meteor": 0.0,
        "sbert_similarity": 0.3672371506690979
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "When did Melanie get hurt?",
      "prediction": "September 2023",
      "reference": "September 2023",
      "category": 2,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 0.9999999403953552,
        "bert_recall": 0.9999999403953552,
        "bert_f1": 0.9999999403953552,
        "meteor": 0.9375,
        "sbert_similarity": 1.000000238418579
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "When did Melanie's family go on a roadtrip?",
      "prediction": "20 October, 2023",
      "reference": "The weekend before 20 October 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5714285714285715,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.45489799478447507,
        "bleu2": 0.30326532985631666,
        "bleu3": 0.14283448174223617,
        "bleu4": 0.11404605374835301,
        "bert_precision": 0.9243854284286499,
        "bert_recall": 0.8825998306274414,
        "bert_f1": 0.9030094742774963,
        "meteor": 0.17543859649122806,
        "sbert_similarity": 0.833219587802887
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "How many children does Melanie have?",
      "prediction": "Melanie's kids are so excited about summer break!",
      "reference": "3",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7899295687675476,
        "bert_recall": 0.8297573924064636,
        "bert_f1": 0.8093538284301758,
        "meteor": 0.0,
        "sbert_similarity": 0.028898779302835464
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "When did Melanie go on a hike after the roadtrip?",
      "prediction": "yesterday",
      "reference": "19 October 2023",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8904600143432617,
        "bert_recall": 0.7936490774154663,
        "bert_f1": 0.8392719626426697,
        "meteor": 0.0,
        "sbert_similarity": 0.38866040110588074
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "Would Melanie go on another roadtrip soon?",
      "prediction": "I was really scared when we got into the accident.",
      "reference": "Likely no; since this one went badly",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8791968822479248,
        "bert_recall": 0.8334329724311829,
        "bert_f1": 0.8557035326957703,
        "meteor": 0.0684931506849315,
        "sbert_similarity": 0.05388104170560837
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What items has Melanie bought?",
      "prediction": "these figurines I bought yesterday",
      "reference": "Figurines, shoes",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.28571428571428575,
        "rouge1_f": 0.28571428571428575,
        "rouge2_f": 0.0,
        "rougeL_f": 0.28571428571428575,
        "bleu1": 0.2,
        "bleu2": 0.07071067811865477,
        "bleu3": 0.05665132701906204,
        "bleu4": 0.05372849659117709,
        "bert_precision": 0.8434215784072876,
        "bert_recall": 0.8121747374534607,
        "bert_f1": 0.8275033235549927,
        "meteor": 0.0,
        "sbert_similarity": 0.6789911389350891
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "When did Caroline pass the adoption interview?",
      "prediction": "22 October 2023",
      "reference": "The Friday before 22 October 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5714285714285715,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.36787944117144233,
        "bleu2": 0.36787944117144233,
        "bleu3": 0.36787944117144233,
        "bleu4": 0.20687381245863395,
        "bert_precision": 0.9592811465263367,
        "bert_recall": 0.879981517791748,
        "bert_f1": 0.9179218411445618,
        "meteor": 0.5165692007797271,
        "sbert_similarity": 0.8485541939735413
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "When did Melanie buy the figurines?",
      "prediction": "22 October 2023",
      "reference": "21 October 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.6666666666666666,
        "bleu2": 0.5773502691896257,
        "bleu3": 0.3254992452916931,
        "bleu4": 0.24028114141347542,
        "bert_precision": 0.9986071586608887,
        "bert_recall": 0.9986071586608887,
        "bert_f1": 0.9986071586608887,
        "meteor": 0.625,
        "sbert_similarity": 0.9786192178726196
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "Would Caroline want to move back to her home country soon?",
      "prediction": "I want to give a loving home to kids who need it.",
      "reference": "No; she's in the process of adopting children.",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0.07692307692307693,
        "bleu2": 0.025318484177091663,
        "bleu3": 0.018733084686676876,
        "bleu4": 0.01553712569276035,
        "bert_precision": 0.8692851066589355,
        "bert_recall": 0.8872348666191101,
        "bert_f1": 0.8781682848930359,
        "meteor": 0.0,
        "sbert_similarity": 0.23490969836711884
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What did the charity race raise awareness for?",
      "prediction": "raising awareness for mental health",
      "reference": "mental health",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5714285714285715,
        "rouge1_f": 0.5714285714285715,
        "rouge2_f": 0.4,
        "rougeL_f": 0.5714285714285715,
        "bleu1": 0.4,
        "bleu2": 0.316227766016838,
        "bleu3": 0.1522474354853864,
        "bleu4": 0.11362193664674995,
        "bert_precision": 0.8476505875587463,
        "bert_recall": 0.9005824327468872,
        "bert_f1": 0.873315155506134,
        "meteor": 0.8152173913043478,
        "sbert_similarity": 0.6556979417800903
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What did Melanie realize after the charity race?",
      "prediction": "self-care is really important. It's a journey for me, but when I look after myself, I'm able to better look after my family.",
      "reference": "self-care is important",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.2666666666666667,
        "rouge2_f": 0.14285714285714285,
        "rougeL_f": 0.2666666666666667,
        "bleu1": 0.10344827586206899,
        "bleu2": 0.06078306738548309,
        "bleu3": 0.024828271296253392,
        "bleu4": 0.0151463165231073,
        "bert_precision": 0.8586325645446777,
        "bert_recall": 0.9606246948242188,
        "bert_f1": 0.9067696332931519,
        "meteor": 0.375,
        "sbert_similarity": 0.8000196218490601
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "How does Melanie prioritize self-care?",
      "prediction": "I'm carving out some me-time each day - running, reading, or playing my violin - which refreshes me and helps me stay present for my fam!",
      "reference": "by carving out some me-time each day for activities like running, reading, or playing the violin",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6153846153846153,
        "rouge1_f": 0.6046511627906976,
        "rouge2_f": 0.43902439024390244,
        "rougeL_f": 0.558139534883721,
        "bleu1": 0.4666666666666667,
        "bleu2": 0.40114777854287703,
        "bleu3": 0.361941551554555,
        "bleu4": 0.3179303998422513,
        "bert_precision": 0.902489423751831,
        "bert_recall": 0.9424307346343994,
        "bert_f1": 0.9220277667045593,
        "meteor": 0.6928104575163399,
        "sbert_similarity": 0.6163120865821838
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What are Caroline's plans for the summer?",
      "prediction": "Gonna continue my edu and check out career options, which is pretty exciting!",
      "reference": "researching adoption agencies",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8084882497787476,
        "bert_recall": 0.8404667377471924,
        "bert_f1": 0.8241674304008484,
        "meteor": 0.0,
        "sbert_similarity": 0.024385549128055573
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What type of individuals does the adoption agency Caroline is considering support?",
      "prediction": "they help LGBTQ+ folks with adoption",
      "reference": "LGBTQ+ individuals",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.16666666666666669,
        "bleu2": 0.05773502691896258,
        "bleu3": 0.04506819749679463,
        "bleu4": 0.040824829046386304,
        "bert_precision": 0.8405470848083496,
        "bert_recall": 0.8698520660400391,
        "bert_f1": 0.8549485802650452,
        "meteor": 0.20833333333333334,
        "sbert_similarity": 0.6111578941345215
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "Why did Caroline choose the adoption agency?",
      "prediction": "I chose them 'cause they help LGBTQ+ folks with adoption. Their inclusivity and support really spoke to me.",
      "reference": "because of their inclusivity and support for LGBTQ+ individuals",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3703703703703704,
        "rouge1_f": 0.3703703703703704,
        "rouge2_f": 0.24,
        "rougeL_f": 0.2962962962962963,
        "bleu1": 0.25,
        "bleu2": 0.19867985355975656,
        "bleu3": 0.16667923575277016,
        "bleu4": 0.12673718536830808,
        "bert_precision": 0.8836987018585205,
        "bert_recall": 0.9078869819641113,
        "bert_f1": 0.8956295847892761,
        "meteor": 0.48888888888888904,
        "sbert_similarity": 0.5311238169670105
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What is Caroline excited about in the adoption process?",
      "prediction": "I'm so excited and thankful. This is a big move towards my goal of having a family.",
      "reference": "creating a family for kids who need one",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.16666666666666666,
        "rouge1_f": 0.15384615384615383,
        "rouge2_f": 0.08333333333333333,
        "rougeL_f": 0.15384615384615383,
        "bleu1": 0.10000000000000002,
        "bleu2": 0.07254762501100116,
        "bleu3": 0.0318985825417419,
        "bleu4": 0.020364851292391,
        "bert_precision": 0.8492698669433594,
        "bert_recall": 0.8326451182365417,
        "bert_f1": 0.8408752679824829,
        "meteor": 0.05617977528089888,
        "sbert_similarity": 0.3787328898906708
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What does Melanie think about Caroline's decision to adopt?",
      "prediction": "Giving a home to needy kids is such a loving way to build a family. Those kids will be so supported and happy in their new home.",
      "reference": "she thinks Caroline is doing something amazing and will be an awesome mom",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2285714285714286,
        "rouge1_f": 0.2,
        "rouge2_f": 0.05263157894736842,
        "rougeL_f": 0.15,
        "bleu1": 0.13793103448275862,
        "bleu2": 0.07018624063435965,
        "bleu3": 0.027300856066018425,
        "bleu4": 0.016275776313613193,
        "bert_precision": 0.8575342893600464,
        "bert_recall": 0.8664098978042603,
        "bert_f1": 0.8619492650032043,
        "meteor": 0.2191840277777778,
        "sbert_similarity": 0.2920159697532654
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "How long have Mel and her husband been married?",
      "prediction": "5 years already!",
      "reference": "Mel and her husband have been married for 5 years.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.30769230769230765,
        "rouge1_f": 0.30769230769230765,
        "rouge2_f": 0.1818181818181818,
        "rougeL_f": 0.30769230769230765,
        "bleu1": 0.08688697172522257,
        "bleu2": 0.07094291534080824,
        "bleu3": 0.035797677029744235,
        "bleu4": 0.029524961852386813,
        "bert_precision": 0.8518999814987183,
        "bert_recall": 0.8372284173965454,
        "bert_f1": 0.8445005416870117,
        "meteor": 0.053763440860215055,
        "sbert_similarity": 0.2896970808506012
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What does Caroline's necklace symbolize?",
      "prediction": "it stands for love, faith and strength.",
      "reference": "love, faith, and strength",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7272727272727273,
        "rouge1_f": 0.7272727272727273,
        "rouge2_f": 0.6666666666666666,
        "rougeL_f": 0.7272727272727273,
        "bleu1": 0.5555555555555556,
        "bleu2": 0.45643546458763845,
        "bleu3": 0.3135508904498039,
        "bleu4": 0.14923729480049117,
        "bert_precision": 0.8965362310409546,
        "bert_recall": 0.9382737874984741,
        "bert_f1": 0.9169302582740784,
        "meteor": 0.23255813953488375,
        "sbert_similarity": 0.6422092914581299
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What country is Caroline's grandma from?",
      "prediction": "Sweden",
      "reference": "Sweden",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9999999403953552,
        "bert_recall": 0.9999999403953552,
        "bert_f1": 0.9999999403953552,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What was grandma's gift to Caroline?",
      "prediction": "a gift from my grandma in my home country, Sweden. She gave it to me when I was young, and it stands for love, faith and strength.",
      "reference": "necklace",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7893295288085938,
        "bert_recall": 0.8171942830085754,
        "bert_f1": 0.8030202388763428,
        "meteor": 0.0,
        "sbert_similarity": 0.4436558485031128
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What is Melanie's hand-painted bowl a reminder of?",
      "prediction": "It reminds me of art and self-expression.",
      "reference": "art and self-expression",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.6,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.375,
        "bleu2": 0.32732683535398854,
        "bleu3": 0.2649097550589377,
        "bleu4": 0.1374708101760565,
        "bert_precision": 0.8986807465553284,
        "bert_recall": 0.9339214563369751,
        "bert_f1": 0.9159622192382812,
        "meteor": 0.5514705882352942,
        "sbert_similarity": 0.6897171139717102
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What did Melanie and her family do while camping?",
      "prediction": "We roasted marshmallows, told stories around the campfire and just enjoyed each other's company.",
      "reference": "explored nature, roasted marshmallows, and went on a hike",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2608695652173913,
        "rouge1_f": 0.25,
        "rouge2_f": 0.09090909090909091,
        "rougeL_f": 0.25,
        "bleu1": 0.23529411764705885,
        "bleu2": 0.17149858514250885,
        "bleu3": 0.1277914629322513,
        "bleu4": 0.061175206929160446,
        "bert_precision": 0.8907185792922974,
        "bert_recall": 0.9045258164405823,
        "bert_f1": 0.8975690603256226,
        "meteor": 0.2690058479532164,
        "sbert_similarity": 0.6077731251716614
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What kind of counseling and mental health services is Caroline interested in pursuing?",
      "prediction": "I'm thinking of working with trans people, helping them accept themselves and supporting their mental health.",
      "reference": "working with trans people, helping them accept themselves and supporting their mental health",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.896551724137931,
        "rouge1_f": 0.8666666666666666,
        "rouge2_f": 0.8571428571428571,
        "rougeL_f": 0.8666666666666666,
        "bleu1": 0.7368421052631579,
        "bleu2": 0.7294955398698874,
        "bleu3": 0.7238967222455629,
        "bleu4": 0.7128736748247595,
        "bert_precision": 0.9448204040527344,
        "bert_recall": 0.9558389186859131,
        "bert_f1": 0.9502977728843689,
        "meteor": 0.9019945697577276,
        "sbert_similarity": 0.9358103275299072
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What workshop did Caroline attend recently?",
      "prediction": "I went to an LGBTQ+ counseling workshop and it was really enlightening.",
      "reference": "LGBTQ+ counseling workshop",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.3076923076923077,
        "rougeL_f": 0.4,
        "bleu1": 0.23076923076923078,
        "bleu2": 0.19611613513818407,
        "bleu3": 0.15466755103052704,
        "bleu4": 0.0768968397262906,
        "bert_precision": 0.8753576874732971,
        "bert_recall": 0.9028002023696899,
        "bert_f1": 0.8888671398162842,
        "meteor": 0.7549857549857552,
        "sbert_similarity": 0.7861557006835938
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What was discussed in the LGBTQ+ counseling workshop?",
      "prediction": "They talked about different therapeutic methods and how to best work with trans people.",
      "reference": "therapeutic methods and how to best work with trans people",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8333333333333333,
        "rouge1_f": 0.8333333333333333,
        "rouge2_f": 0.8181818181818181,
        "rougeL_f": 0.8333333333333333,
        "bleu1": 0.6666666666666666,
        "bleu2": 0.6546536707079771,
        "bleu3": 0.6441485748898267,
        "bleu4": 0.6262844962765469,
        "bert_precision": 0.9290652871131897,
        "bert_recall": 0.9489880800247192,
        "bert_f1": 0.9389210343360901,
        "meteor": 0.8647910731244065,
        "sbert_similarity": 0.8247993588447571
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What motivated Caroline to pursue counseling?",
      "prediction": "My own journey and the support I got made a huge difference.",
      "reference": "her own journey and the support she received, and how counseling improved her life",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4166666666666667,
        "rouge1_f": 0.3846153846153846,
        "rouge2_f": 0.33333333333333337,
        "rougeL_f": 0.3846153846153846,
        "bleu1": 0.3297707381386313,
        "bleu2": 0.30699984978524775,
        "bleu3": 0.2835206353227474,
        "bleu4": 0.24794277351227223,
        "bert_precision": 0.9339774250984192,
        "bert_recall": 0.9229704737663269,
        "bert_f1": 0.9284413456916809,
        "meteor": 0.32318840579710145,
        "sbert_similarity": 0.47101399302482605
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What kind of place does Caroline want to create for people?",
      "prediction": "a safe and loving home for these kids",
      "reference": "a safe and inviting place for people to grow",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.47058823529411764,
        "rouge1_f": 0.47058823529411764,
        "rouge2_f": 0.26666666666666666,
        "rougeL_f": 0.47058823529411764,
        "bleu1": 0.4412484512922977,
        "bleu2": 0.3335524767176619,
        "bleu3": 0.257063800471676,
        "bleu4": 0.13036420666577953,
        "bert_precision": 0.9078570604324341,
        "bert_recall": 0.902176022529602,
        "bert_f1": 0.9050076007843018,
        "meteor": 0.5438202247191011,
        "sbert_similarity": 0.5048529505729675
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "Did Melanie make the black and white bowl in the photo?",
      "prediction": "Yeah, I made that bowl in my class. It took some work, but I'm pretty proud of it.",
      "reference": "Yes",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8107691407203674,
        "bert_recall": 0.8312549591064453,
        "bert_f1": 0.8208842873573303,
        "meteor": 0.0,
        "sbert_similarity": 0.05468305945396423
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What kind of books does Caroline have in her library?",
      "prediction": "lots of kids' books- classics, stories from different cultures, educational books, all of that.",
      "reference": "kids' books - classics, stories from different cultures, educational books",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7272727272727274,
        "rouge1_f": 0.782608695652174,
        "rouge2_f": 0.761904761904762,
        "rougeL_f": 0.782608695652174,
        "bleu1": 0.5789473684210527,
        "bleu2": 0.5380275868489703,
        "bleu3": 0.4956386990444981,
        "bleu4": 0.4598036015897535,
        "bert_precision": 0.9285295605659485,
        "bert_recall": 0.9537140130996704,
        "bert_f1": 0.940953254699707,
        "meteor": 0.6652276295133438,
        "sbert_similarity": 0.8430535793304443
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What was Melanie's favorite book from her childhood?",
      "prediction": "\"Charlotte's Web\" as a kid.",
      "reference": "\"Charlotte's Web\"",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5714285714285715,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5714285714285715,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.5555555555555556,
        "bleu2": 0.5270462766947299,
        "bleu3": 0.49543631365378615,
        "bleu4": 0.44632361378533286,
        "bert_precision": 0.9301528930664062,
        "bert_recall": 0.9561277031898499,
        "bert_f1": 0.9429614543914795,
        "meteor": 0.8152173913043478,
        "sbert_similarity": 0.8913426995277405
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What book did Caroline recommend to Melanie?",
      "prediction": "\"Becoming Nicole\" by Amy Ellis Nutt.",
      "reference": "\"Becoming Nicole\"",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.33333333333333337,
        "rougeL_f": 0.5,
        "bleu1": 0.4444444444444444,
        "bleu2": 0.408248290463863,
        "bleu3": 0.3661572458236839,
        "bleu4": 0.2984745896009823,
        "bert_precision": 0.8811114430427551,
        "bert_recall": 0.9694058895111084,
        "bert_f1": 0.9231523275375366,
        "meteor": 0.78125,
        "sbert_similarity": 0.7488878965377808
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What did Caroline take away from the book \"Becoming Nicole\"?",
      "prediction": "It taught me self-acceptance and how to find support.",
      "reference": "Lessons on self-acceptance and finding support",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.588235294117647,
        "rouge2_f": 0.4,
        "rougeL_f": 0.588235294117647,
        "bleu1": 0.3,
        "bleu2": 0.18257418583505533,
        "bleu3": 0.07665320007195604,
        "bleu4": 0.04939382737115371,
        "bert_precision": 0.924019992351532,
        "bert_recall": 0.9420633912086487,
        "bert_f1": 0.9329544901847839,
        "meteor": 0.40564373897707234,
        "sbert_similarity": 0.7891430854797363
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What are the new shoes that Melanie got used for?",
      "prediction": "These are for running.",
      "reference": "Running",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.2,
        "bleu2": 0.07071067811865477,
        "bleu3": 0.05665132701906204,
        "bleu4": 0.05372849659117709,
        "bert_precision": 0.8325475454330444,
        "bert_recall": 0.8574665188789368,
        "bert_f1": 0.8448233008384705,
        "meteor": 0.0,
        "sbert_similarity": 0.6294575929641724
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What is Melanie's reason for getting into running?",
      "prediction": "I've been running farther to de-stress, which has been great for my headspace.",
      "reference": "To de-stress and clear her mind",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2222222222222222,
        "rouge1_f": 0.27272727272727276,
        "rouge2_f": 0.2,
        "rougeL_f": 0.27272727272727276,
        "bleu1": 0.12500000000000003,
        "bleu2": 0.09128709291752769,
        "bleu3": 0.04033183084278514,
        "bleu4": 0.026012784404037925,
        "bert_precision": 0.8737565279006958,
        "bert_recall": 0.9166405200958252,
        "bert_f1": 0.8946849703788757,
        "meteor": 0.07462686567164178,
        "sbert_similarity": 0.3734774589538574
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What does Melanie say running has been great for?",
      "prediction": "great for my mental health.",
      "reference": "Her mental health",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.3333333333333333,
        "rougeL_f": 0.5,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.25819888974716115,
        "bleu3": 0.12111838948674812,
        "bleu4": 0.08633400213704504,
        "bert_precision": 0.9201779365539551,
        "bert_recall": 0.9352365732192993,
        "bert_f1": 0.9276461601257324,
        "meteor": 0.15625,
        "sbert_similarity": 0.4702323079109192
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What did Mel and her kids make during the pottery workshop?",
      "prediction": "they all made their own pots",
      "reference": "pots",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2857142857142857,
        "rouge1_f": 0.2857142857142857,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2857142857142857,
        "bleu1": 0.16666666666666669,
        "bleu2": 0.05773502691896258,
        "bleu3": 0.04506819749679463,
        "bleu4": 0.040824829046386304,
        "bert_precision": 0.8119804859161377,
        "bert_recall": 0.8834489583969116,
        "bert_f1": 0.846208393573761,
        "meteor": 0.33333333333333337,
        "sbert_similarity": 0.6146188974380493
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What kind of pot did Mel and her kids make with clay?",
      "prediction": "They all made their own pots.",
      "reference": "a cup with a dog face on it",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.884564995765686,
        "bert_recall": 0.8538767099380493,
        "bert_f1": 0.8689500093460083,
        "meteor": 0.0,
        "sbert_similarity": 0.05987963452935219
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What creative project do Mel and her kids do together besides pottery?",
      "prediction": "We love painting together lately, especially nature-inspired ones.",
      "reference": "painting",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2222222222222222,
        "rouge1_f": 0.19999999999999998,
        "rouge2_f": 0.0,
        "rougeL_f": 0.19999999999999998,
        "bleu1": 0.10000000000000002,
        "bleu2": 0.03333333333333335,
        "bleu3": 0.024950558772614845,
        "bleu4": 0.021105340631872645,
        "bert_precision": 0.8177852630615234,
        "bert_recall": 0.8406197428703308,
        "bert_f1": 0.8290452361106873,
        "meteor": 0.2941176470588236,
        "sbert_similarity": 0.6080063581466675
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What did Mel and her kids paint in their latest project in July 2023?",
      "prediction": "nature-inspired ones",
      "reference": "a sunset with a palm tree",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8419816493988037,
        "bert_recall": 0.8387597799301147,
        "bert_f1": 0.8403676152229309,
        "meteor": 0.0,
        "sbert_similarity": 0.22545376420021057
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What did Caroline see at the council meeting for adoption?",
      "prediction": "it was inspiring and emotional - so many people wanted to create loving homes for children in need.",
      "reference": "many people wanting to create loving homes for children in need",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6896551724137931,
        "rouge1_f": 0.7857142857142858,
        "rouge2_f": 0.7692307692307693,
        "rougeL_f": 0.7857142857142858,
        "bleu1": 0.5263157894736842,
        "bleu2": 0.4836508334066744,
        "bleu3": 0.4390690748074934,
        "bleu4": 0.4007781700028195,
        "bert_precision": 0.9040247201919556,
        "bert_recall": 0.9486033916473389,
        "bert_f1": 0.9257777333259583,
        "meteor": 0.8542735042735045,
        "sbert_similarity": 0.7657339572906494
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What do sunflowers represent according to Caroline?",
      "prediction": "Sunflowers mean warmth and happiness",
      "reference": "warmth and happiness",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7499999999999999,
        "rouge1_f": 0.7499999999999999,
        "rouge2_f": 0.6666666666666666,
        "rougeL_f": 0.7499999999999999,
        "bleu1": 0.6,
        "bleu2": 0.5477225575051662,
        "bleu3": 0.4677351412871982,
        "bleu4": 0.26591479484724945,
        "bert_precision": 0.829066276550293,
        "bert_recall": 0.9040235280990601,
        "bert_f1": 0.8649239540100098,
        "meteor": 0.920138888888889,
        "sbert_similarity": 0.5833069086074829
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "Why are flowers important to Melanie?",
      "prediction": "Flowers bring joy. They represent growth, beauty and reminding us to appreciate the small moments.",
      "reference": "They remind her to appreciate the small moments and were a part of her wedding decor",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4666666666666667,
        "rouge1_f": 0.5161290322580646,
        "rouge2_f": 0.2758620689655172,
        "rougeL_f": 0.45161290322580644,
        "bleu1": 0.3888888888888889,
        "bleu2": 0.30249507099101003,
        "bleu3": 0.26143546377963406,
        "bleu4": 0.21869766863251183,
        "bert_precision": 0.9000487327575684,
        "bert_recall": 0.8739659786224365,
        "bert_f1": 0.88681560754776,
        "meteor": 0.39917853934026437,
        "sbert_similarity": 0.5231173038482666
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What inspired Caroline's painting for the art show?",
      "prediction": "I wanted to capture everyone's unity and strength.",
      "reference": "visiting an LGBTQ center and wanting to capture unity and strength",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5555555555555556,
        "rouge1_f": 0.6,
        "rouge2_f": 0.4444444444444445,
        "rougeL_f": 0.6,
        "bleu1": 0.45241870901797976,
        "bleu2": 0.36939832906091624,
        "bleu3": 0.25220921097920224,
        "bleu4": 0.11884631831419354,
        "bert_precision": 0.9372824430465698,
        "bert_recall": 0.9003862142562866,
        "bert_f1": 0.918463945388794,
        "meteor": 0.4523364485981308,
        "sbert_similarity": 0.4163987636566162
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "How often does Melanie go to the beach with her kids?",
      "prediction": "usually only once or twice a year",
      "reference": "once or twice a year",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8333333333333333,
        "rouge1_f": 0.8333333333333333,
        "rouge2_f": 0.8,
        "rougeL_f": 0.8333333333333333,
        "bleu1": 0.7142857142857143,
        "bleu2": 0.6900655593423543,
        "bleu3": 0.6613898798842307,
        "bleu4": 0.6147881529512643,
        "bert_precision": 0.9254474639892578,
        "bert_recall": 0.9496262073516846,
        "bert_f1": 0.9373809695243835,
        "meteor": 0.9576923076923077,
        "sbert_similarity": 0.9151639938354492
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What did Melanie and her family see during their camping trip last year?",
      "prediction": "I'll always remember our camping trip last year when we saw the Perseid meteor shower.",
      "reference": "Perseid meteor shower",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.33333333333333337,
        "rouge1_f": 0.3157894736842105,
        "rouge2_f": 0.23529411764705882,
        "rougeL_f": 0.3157894736842105,
        "bleu1": 0.17647058823529413,
        "bleu2": 0.14852213144650114,
        "bleu3": 0.11621764179681902,
        "bleu4": 0.05692994482485313,
        "bert_precision": 0.836280345916748,
        "bert_recall": 0.9132500290870667,
        "bert_f1": 0.87307208776474,
        "meteor": 0.4464285714285714,
        "sbert_similarity": 0.7110863924026489
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "How did Melanie feel while watching the meteor shower?",
      "prediction": "It was one of those moments where I felt tiny and in awe of the universe.",
      "reference": "in awe of the universe",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.47619047619047616,
        "rouge2_f": 0.4210526315789474,
        "rougeL_f": 0.47619047619047616,
        "bleu1": 0.29411764705882354,
        "bleu2": 0.2711630722733202,
        "bleu3": 0.24846891229294918,
        "bleu4": 0.21409092659758044,
        "bert_precision": 0.8618944883346558,
        "bert_recall": 0.9205403923988342,
        "bert_f1": 0.8902526497840881,
        "meteor": 0.6506147540983607,
        "sbert_similarity": 0.5921179056167603
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "Whose birthday did Melanie celebrate recently?",
      "prediction": "my daughter's birthday",
      "reference": "Melanie's daughter",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.5714285714285715,
        "rouge2_f": 0.0,
        "rougeL_f": 0.28571428571428575,
        "bleu1": 0.5,
        "bleu2": 0.12909944487358058,
        "bleu3": 0.09635409769034631,
        "bleu4": 0.09554427922043669,
        "bert_precision": 0.8833531737327576,
        "bert_recall": 0.9120132327079773,
        "bert_f1": 0.8974544405937195,
        "meteor": 0.0,
        "sbert_similarity": 0.445888876914978
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "Who performed at the concert at Melanie's daughter's birthday?",
      "prediction": "Matt Patterson",
      "reference": "Matt Patterson",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9375,
        "sbert_similarity": 1.0
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "Why did Melanie choose to use colors and patterns in her pottery project?",
      "prediction": "I made something to catch the eye and make people smile.",
      "reference": "She wanted to catch the eye and make people smile.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.761904761904762,
        "rouge1_f": 0.761904761904762,
        "rouge2_f": 0.7368421052631577,
        "rougeL_f": 0.761904761904762,
        "bleu1": 0.75,
        "bleu2": 0.7385489458759964,
        "bleu3": 0.7278010532191317,
        "bleu4": 0.7102992180127422,
        "bert_precision": 0.9646799564361572,
        "bert_recall": 0.9721308946609497,
        "bert_f1": 0.9683910608291626,
        "meteor": 0.7913056930693071,
        "sbert_similarity": 0.5822879076004028
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What pet does Caroline have?",
      "prediction": "Oscar, my guinea pig.",
      "reference": "guinea pig",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.25819888974716115,
        "bleu3": 0.12111838948674812,
        "bleu4": 0.08633400213704504,
        "bert_precision": 0.8800321817398071,
        "bert_recall": 0.8927434682846069,
        "bert_f1": 0.8863422274589539,
        "meteor": 0.22727272727272727,
        "sbert_similarity": 0.6781593561172485
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What pets does Melanie have?",
      "prediction": "we got a pup and a kitty; they brighten up our day and always make us smile.",
      "reference": "Two cats and a dog",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2,
        "rouge1_f": 0.1818181818181818,
        "rouge2_f": 0.1,
        "rougeL_f": 0.1818181818181818,
        "bleu1": 0.10526315789473682,
        "bleu2": 0.07647191129018724,
        "bleu3": 0.033656049636627425,
        "bleu4": 0.02153320854081468,
        "bert_precision": 0.8410614728927612,
        "bert_recall": 0.8880242109298706,
        "bert_f1": 0.863905131816864,
        "meteor": 0.16129032258064516,
        "sbert_similarity": 0.49601423740386963
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "Where did Oliver hide his bone once?",
      "prediction": "He hid his bone in my slipper once!",
      "reference": "In Melanie's slipper",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.36363636363636365,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.2222222222222222,
        "bleu2": 0.05270462766947299,
        "bleu3": 0.03528075103412339,
        "bleu4": 0.028517539529041507,
        "bert_precision": 0.8625865578651428,
        "bert_recall": 0.8832314014434814,
        "bert_f1": 0.8727869391441345,
        "meteor": 0.28571428571428575,
        "sbert_similarity": 0.4252103567123413
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What activity did Caroline used to do with her dad?",
      "prediction": "I used to go horseback riding with my dad when I was a kid",
      "reference": "Horseback riding",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2666666666666667,
        "rouge1_f": 0.25,
        "rouge2_f": 0.14285714285714288,
        "rougeL_f": 0.25,
        "bleu1": 0.14285714285714285,
        "bleu2": 0.10482848367219184,
        "bleu3": 0.04649288856043541,
        "bleu4": 0.030206205252435548,
        "bert_precision": 0.821826696395874,
        "bert_recall": 0.8849934935569763,
        "bert_f1": 0.8522411584854126,
        "meteor": 0.5859375000000001,
        "sbert_similarity": 0.660359263420105
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What did Caroline make for a local church?",
      "prediction": "I made this stained glass window to remind myself and others that within us all is the key to discovering our true potential and living our best life.",
      "reference": "a stained glass window",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.20689655172413793,
        "rouge1_f": 0.1875,
        "rouge2_f": 0.13333333333333333,
        "rougeL_f": 0.1875,
        "bleu1": 0.10344827586206899,
        "bleu2": 0.08596023825918794,
        "bleu3": 0.06672466173860504,
        "bleu4": 0.032030559677017297,
        "bert_precision": 0.8255722522735596,
        "bert_recall": 0.9400502443313599,
        "bert_f1": 0.8791000247001648,
        "meteor": 0.4600694444444445,
        "sbert_similarity": 0.5781451463699341
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What did Caroline find in her neighborhood during her walk?",
      "prediction": "this cool rainbow sidewalk for Pride Month",
      "reference": "a rainbow sidewalk",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.25,
        "rougeL_f": 0.4,
        "bleu1": 0.2857142857142857,
        "bleu2": 0.2182178902359924,
        "bleu3": 0.1006949130683619,
        "bleu4": 0.06985342056580097,
        "bert_precision": 0.881127119064331,
        "bert_recall": 0.9606530070304871,
        "bert_f1": 0.9191731214523315,
        "meteor": 0.5514705882352942,
        "sbert_similarity": 0.7868509888648987
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "Which song motivates Caroline to be courageous?",
      "prediction": "\"Brave\" by Sara Bareilles has a lot of significance for me. It's about being courageous and fighting for what's right.",
      "reference": "Brave by Sara Bareilles",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2608695652173913,
        "rouge1_f": 0.3076923076923077,
        "rouge2_f": 0.25,
        "rougeL_f": 0.3076923076923077,
        "bleu1": 0.15384615384615385,
        "bleu2": 0.11094003924504585,
        "bleu3": 0.08208968293684403,
        "bleu4": 0.03864198149136354,
        "bert_precision": 0.8493939638137817,
        "bert_recall": 0.9284552931785583,
        "bert_f1": 0.8871667385101318,
        "meteor": 0.5257936507936508,
        "sbert_similarity": 0.6410201191902161
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "Which  classical musicians does Melanie enjoy listening to?",
      "prediction": "classical like Bach and Mozart",
      "reference": "Bach and Mozart",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7499999999999999,
        "rouge1_f": 0.7499999999999999,
        "rouge2_f": 0.6666666666666666,
        "rougeL_f": 0.7499999999999999,
        "bleu1": 0.6,
        "bleu2": 0.5477225575051662,
        "bleu3": 0.4677351412871982,
        "bleu4": 0.26591479484724945,
        "bert_precision": 0.9011482000350952,
        "bert_recall": 0.9204593896865845,
        "bert_f1": 0.9107014536857605,
        "meteor": 0.920138888888889,
        "sbert_similarity": 0.8429141640663147
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "Who is Melanie a fan of in terms of modern music?",
      "prediction": "modern music like Ed Sheeran's \"Perfect\"",
      "reference": "Ed Sheeran",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.4444444444444445,
        "rouge2_f": 0.2857142857142857,
        "rougeL_f": 0.4444444444444445,
        "bleu1": 0.2222222222222222,
        "bleu2": 0.16666666666666669,
        "bleu3": 0.07542890820012248,
        "bleu4": 0.050712153369465586,
        "bert_precision": 0.8577525615692139,
        "bert_recall": 0.9161736965179443,
        "bert_f1": 0.8860011696815491,
        "meteor": 0.20833333333333334,
        "sbert_similarity": 0.6261199712753296
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "How long has Melanie been creating art?",
      "prediction": "Seven years now",
      "reference": "7 years",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.12909944487358058,
        "bleu3": 0.12111838948674812,
        "bleu4": 0.11362193664674995,
        "bert_precision": 0.9228770732879639,
        "bert_recall": 0.9635454416275024,
        "bert_f1": 0.9427728652954102,
        "meteor": 0.8928571428571429,
        "sbert_similarity": 0.8659797310829163
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What precautionary sign did Melanie see at the caf\u00e9?",
      "prediction": "the sign was just a precaution",
      "reference": "A sign stating that someone is not being able to leave",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.23529411764705885,
        "rouge1_f": 0.23529411764705885,
        "rouge2_f": 0.0,
        "rougeL_f": 0.11764705882352942,
        "bleu1": 0.14486606950235942,
        "bleu2": 0.03548479513233444,
        "bleu3": 0.024620565232033,
        "bleu4": 0.021099385422893017,
        "bert_precision": 0.8708813190460205,
        "bert_recall": 0.8462989926338196,
        "bert_f1": 0.8584142327308655,
        "meteor": 0.09523809523809525,
        "sbert_similarity": 0.5227034091949463
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What advice does Caroline give for getting started with adoption?",
      "prediction": "Do your research and find an adoption agency or lawyer. They'll help with the process and provide all the info. Gather documents like references, financial info and medical checks. Don't forget to prepare emotionally, since the wait can be hard.",
      "reference": "Do research, find an adoption agency or lawyer, gather necessary documents, and prepare emotionally.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5306122448979592,
        "rouge1_f": 0.46428571428571436,
        "rouge2_f": 0.2222222222222222,
        "rougeL_f": 0.46428571428571436,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.2062842492517587,
        "bleu3": 0.15758588611896324,
        "bleu4": 0.12532443621763048,
        "bert_precision": 0.8881880640983582,
        "bert_recall": 0.9453722238540649,
        "bert_f1": 0.9158884882926941,
        "meteor": 0.49568644950171065,
        "sbert_similarity": 0.8916806578636169
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What setback did Melanie face in October 2023?",
      "prediction": "last month I got hurt and had to take a break from pottery",
      "reference": "She got hurt and had to take a break from pottery.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8333333333333333,
        "rouge1_f": 0.8333333333333333,
        "rouge2_f": 0.8181818181818182,
        "rougeL_f": 0.8333333333333333,
        "bleu1": 0.7692307692307693,
        "bleu2": 0.75955452531275,
        "bleu3": 0.7508082226694607,
        "bleu4": 0.7361703354503866,
        "bert_precision": 0.9596830606460571,
        "bert_recall": 0.9773361682891846,
        "bert_f1": 0.9684291481971741,
        "meteor": 0.8030202821869489,
        "sbert_similarity": 0.6912587881088257
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What does Melanie do to keep herself busy during her pottery break?",
      "prediction": "I've been reading that book you recommended a while ago and painting to keep busy.",
      "reference": "Read a book and paint.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3,
        "rouge1_f": 0.47619047619047616,
        "rouge2_f": 0.10526315789473685,
        "rougeL_f": 0.38095238095238093,
        "bleu1": 0.23529411764705885,
        "bleu2": 0.03834824944236853,
        "bleu3": 0.022241390914229782,
        "bleu4": 0.01626739260030573,
        "bert_precision": 0.8556889295578003,
        "bert_recall": 0.9068709015846252,
        "bert_f1": 0.8805367350578308,
        "meteor": 0.3333333333333334,
        "sbert_similarity": 0.4805826246738434
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What painting did Melanie show to Caroline on October 13, 2023?",
      "prediction": "Here's one I did last week. It's inspired by the sunsets. The colors make me feel calm.",
      "reference": "A painting inspired by sunsets with a pink sky.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.21428571428571427,
        "rouge2_f": 0.07692307692307691,
        "rougeL_f": 0.21428571428571427,
        "bleu1": 0.18181818181818182,
        "bleu2": 0.0930484210398471,
        "bleu3": 0.03630847819655544,
        "bleu4": 0.021847844937497602,
        "bert_precision": 0.8938379883766174,
        "bert_recall": 0.915255606174469,
        "bert_f1": 0.9044199585914612,
        "meteor": 0.19132653061224492,
        "sbert_similarity": 0.4723145365715027
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What kind of painting did Caroline share with Melanie on October 13, 2023?",
      "prediction": "It was a drawing of a woman in a dress.",
      "reference": "An abstract painting with blue streaks on a wall.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.11764705882352941,
        "rouge1_f": 0.10526315789473685,
        "rouge2_f": 0.0,
        "rougeL_f": 0.10526315789473685,
        "bleu1": 0.18181818181818182,
        "bleu2": 0.042640143271122095,
        "bleu3": 0.028234511901075607,
        "bleu4": 0.022416933501922302,
        "bert_precision": 0.9052697420120239,
        "bert_recall": 0.8890692591667175,
        "bert_f1": 0.8970963954925537,
        "meteor": 0.05494505494505494,
        "sbert_similarity": 0.276139497756958
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What was the poetry reading that Caroline attended about?",
      "prediction": "It was a transgender poetry reading where transgender people shared their stories through poetry.",
      "reference": "It was a transgender poetry reading where transgender people shared their stories.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.9565217391304348,
        "rouge1_f": 0.923076923076923,
        "rouge2_f": 0.9166666666666666,
        "rougeL_f": 0.923076923076923,
        "bleu1": 0.8666666666666667,
        "bleu2": 0.8251983888449983,
        "bleu3": 0.8078434700407627,
        "bleu4": 0.7916963878457504,
        "bert_precision": 0.9864804744720459,
        "bert_recall": 0.9956105947494507,
        "bert_f1": 0.9910245537757874,
        "meteor": 0.9013006367700853,
        "sbert_similarity": 0.9938866496086121
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What did the posters at the poetry reading say?",
      "prediction": "trans lives matter",
      "reference": "\"Trans Lives Matter\"",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 0.513417119032592,
        "bleu2": 0.513417119032592,
        "bleu3": 0.513417119032592,
        "bleu4": 0.288715663092199,
        "bert_precision": 0.9173698425292969,
        "bert_recall": 0.8874611854553223,
        "bert_f1": 0.9021676778793335,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.9525077939033508
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What does Caroline's drawing symbolize for her?",
      "prediction": "It stands for freedom and being real.",
      "reference": "Freedom and being true to herself.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4615384615384615,
        "rouge1_f": 0.4615384615384615,
        "rouge2_f": 0.3636363636363636,
        "rougeL_f": 0.4615384615384615,
        "bleu1": 0.5,
        "bleu2": 0.3779644730092272,
        "bleu3": 0.29129144784395894,
        "bleu4": 0.1477219991186121,
        "bert_precision": 0.8768472671508789,
        "bert_recall": 0.8911443948745728,
        "bert_f1": 0.8839380145072937,
        "meteor": 0.4826958105646631,
        "sbert_similarity": 0.3979475498199463
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "How do Melanie and Caroline describe their journey through life together?",
      "prediction": "It's an ongoing adventure of learning and growing.",
      "reference": "An ongoing adventure of learning and growing.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.9333333333333333,
        "rouge1_f": 0.8750000000000001,
        "rouge2_f": 0.8571428571428571,
        "rougeL_f": 0.8750000000000001,
        "bleu1": 0.8,
        "bleu2": 0.7888106377466155,
        "bleu3": 0.7776286229411347,
        "bleu4": 0.7598356856515925,
        "bert_precision": 0.9675580263137817,
        "bert_recall": 0.9841487407684326,
        "bert_f1": 0.9757828712463379,
        "meteor": 0.9844782983615982,
        "sbert_similarity": 0.9022247791290283
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What happened to Melanie's son on their road trip?",
      "prediction": "my son got into an accident.",
      "reference": "He got into an accident",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7272727272727272,
        "rouge1_f": 0.7272727272727272,
        "rouge2_f": 0.6666666666666665,
        "rougeL_f": 0.7272727272727272,
        "bleu1": 0.5714285714285714,
        "bleu2": 0.5345224838248488,
        "bleu3": 0.4888069165703076,
        "bleu4": 0.41113361690051975,
        "bert_precision": 0.9515380263328552,
        "bert_recall": 0.9589357376098633,
        "bert_f1": 0.9552225470542908,
        "meteor": 0.5773420479302832,
        "sbert_similarity": 0.7178585529327393
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "How did Melanie's son handle the accident?",
      "prediction": "They were scared but we reassured them and explained their brother would be OK. They're tough kids.",
      "reference": "He was scared but reassured by his family",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.24,
        "rouge1_f": 0.23076923076923078,
        "rouge2_f": 0.08333333333333333,
        "rougeL_f": 0.23076923076923078,
        "bleu1": 0.15789473684210523,
        "bleu2": 0.09365858115816939,
        "bleu3": 0.03847452395409486,
        "bleu4": 0.023830412565256146,
        "bert_precision": 0.8934625387191772,
        "bert_recall": 0.9104354977607727,
        "bert_f1": 0.9018691778182983,
        "meteor": 0.2871410736579275,
        "sbert_similarity": 0.7031127214431763
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "How did Melanie feel about her family after the accident?",
      "prediction": "I've thought a lot about how much I need them. They mean the world to me and I'm so thankful to have them.",
      "reference": "They are important and mean the world to her",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.3529411764705882,
        "rouge2_f": 0.1875,
        "rougeL_f": 0.29411764705882354,
        "bleu1": 0.2222222222222222,
        "bleu2": 0.16012815380508713,
        "bleu3": 0.12970848159574594,
        "bleu4": 0.09615094003919297,
        "bert_precision": 0.8854310512542725,
        "bert_recall": 0.9245287179946899,
        "bert_f1": 0.9045575857162476,
        "meteor": 0.4914529914529915,
        "sbert_similarity": 0.428856760263443
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "How did Melanie's children handle the accident?",
      "prediction": "They were scared but we reassured them and explained their brother would be OK. They're tough kids.",
      "reference": "They were scared but resilient",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3636363636363636,
        "rouge1_f": 0.3478260869565218,
        "rouge2_f": 0.2857142857142857,
        "rougeL_f": 0.3478260869565218,
        "bleu1": 0.21052631578947367,
        "bleu2": 0.1873171623163388,
        "bleu3": 0.16337776502819226,
        "bleu4": 0.12673718536830808,
        "bert_precision": 0.8778966665267944,
        "bert_recall": 0.9136025905609131,
        "bert_f1": 0.8953938484191895,
        "meteor": 0.6401209677419355,
        "sbert_similarity": 0.5933233499526978
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "How did Melanie feel after the accident?",
      "prediction": "I was really scared when we got into the accident. Thankfully, my son's ok and that was a reminder that life is precious and to cherish our family.",
      "reference": "Grateful and thankful for her family",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.12903225806451613,
        "rouge1_f": 0.17142857142857143,
        "rouge2_f": 0.0,
        "rougeL_f": 0.1142857142857143,
        "bleu1": 0.0625,
        "bleu2": 0.014199045856176616,
        "bleu3": 0.009184193658263872,
        "bleu4": 0.006938247178254633,
        "bert_precision": 0.8429199457168579,
        "bert_recall": 0.854417622089386,
        "bert_f1": 0.8486298322677612,
        "meteor": 0.06097560975609757,
        "sbert_similarity": 0.25612547993659973
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What was Melanie's reaction to her children enjoying the Grand Canyon?",
      "prediction": "they enjoyed the Grand Canyon a lot!",
      "reference": "She was happy and thankful",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8625913858413696,
        "bert_recall": 0.8752244710922241,
        "bert_f1": 0.868861973285675,
        "meteor": 0.0,
        "sbert_similarity": 0.20662692189216614
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What do Melanie's family give her?",
      "prediction": "They mean the world to me and I'm so thankful to have them.",
      "reference": "Strength and motivation",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.13333333333333333,
        "rouge1_f": 0.11764705882352941,
        "rouge2_f": 0.0,
        "rougeL_f": 0.11764705882352941,
        "bleu1": 0.06666666666666667,
        "bleu2": 0.02182178902359924,
        "bleu3": 0.016071866876084712,
        "bleu4": 0.013217947626377298,
        "bert_precision": 0.8164498209953308,
        "bert_recall": 0.832343578338623,
        "bert_f1": 0.8243200182914734,
        "meteor": 0.125,
        "sbert_similarity": 0.14594626426696777
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "How did Melanie feel about her family supporting her?",
      "prediction": "they're my biggest motivation and support.",
      "reference": "She appreciated them a lot",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8674759268760681,
        "bert_recall": 0.8831522464752197,
        "bert_f1": 0.8752439022064209,
        "meteor": 0.0,
        "sbert_similarity": 0.24665069580078125
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What did Melanie do after the road trip to relax?",
      "prediction": "It was a nice way to relax after the road trip.",
      "reference": "Went on a nature walk or hike",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.1111111111111111,
        "rouge1_f": 0.1111111111111111,
        "rouge2_f": 0.0,
        "rougeL_f": 0.1111111111111111,
        "bleu1": 0.08333333333333333,
        "bleu2": 0.027524094128159027,
        "bleu3": 0.020427282860665447,
        "bleu4": 0.017033186037639283,
        "bert_precision": 0.8673622608184814,
        "bert_recall": 0.8478658199310303,
        "bert_f1": 0.8575032353401184,
        "meteor": 0.06756756756756757,
        "sbert_similarity": 0.4398045539855957
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What did Caroline realize after her charity race?",
      "prediction": "self-care is important",
      "reference": "self-care is important",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 0.9999997615814209,
        "bert_recall": 0.9999997615814209,
        "bert_f1": 0.9999997615814209,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What are Melanie's plans for the summer with respect to adoption?",
      "prediction": "researching adoption agencies",
      "reference": "researching adoption agencies",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 0.9999998807907104,
        "bert_recall": 0.9999998807907104,
        "bert_f1": 0.9999998807907104,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What type of individuals does the adoption agency Melanie is considering support?",
      "prediction": "LGBTQ+ individuals",
      "reference": "LGBTQ+ individuals",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 0.9999999403953552,
        "bert_recall": 0.9999999403953552,
        "bert_f1": 0.9999999403953552,
        "meteor": 0.9375,
        "sbert_similarity": 1.0000001192092896
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "Why did Melanie choose the adoption agency?",
      "prediction": "because of their inclusivity and support for LGBTQ+ individuals",
      "reference": "because of their inclusivity and support for LGBTQ+ individuals",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.000000238418579,
        "bert_recall": 1.000000238418579,
        "bert_f1": 1.000000238418579,
        "meteor": 0.9993141289437586,
        "sbert_similarity": 1.0
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What is Melanie excited about in her adoption process?",
      "prediction": "Not mentioned in the conversation",
      "reference": "creating a family for kids who need one",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8648310303688049,
        "bert_recall": 0.8169589042663574,
        "bert_f1": 0.8402136564254761,
        "meteor": 0.0,
        "sbert_similarity": -0.0609419085085392
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What does Melanie's necklace symbolize?",
      "prediction": "not mentioned in the conversation",
      "reference": "love, faith, and strength",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8872008323669434,
        "bert_recall": 0.8567842841148376,
        "bert_f1": 0.8717272281646729,
        "meteor": 0.0,
        "sbert_similarity": 0.040116798132658005
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What country is Melanie's grandma from?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Sweden",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8494402766227722,
        "bert_recall": 0.8910871744155884,
        "bert_f1": 0.8697655200958252,
        "meteor": 0.0,
        "sbert_similarity": 0.12741492688655853
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What was grandma's gift to Melanie?",
      "prediction": "Not mentioned in the conversation",
      "reference": "necklace",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8458929657936096,
        "bert_recall": 0.8909749388694763,
        "bert_f1": 0.8678488731384277,
        "meteor": 0.0,
        "sbert_similarity": 0.17644494771957397
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What was grandpa's gift to Caroline?",
      "prediction": "necklace",
      "reference": "necklace",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9999994039535522,
        "bert_recall": 0.9999994039535522,
        "bert_f1": 0.9999994039535522,
        "meteor": 0.5,
        "sbert_similarity": 1.000000238418579
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What is Caroline's hand-painted bowl a reminder of?",
      "prediction": "art and self-expression",
      "reference": "art and self-expression",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 0.9999998807907104,
        "bert_recall": 0.9999998807907104,
        "bert_f1": 0.9999998807907104,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What did Caroline and her family do while camping?",
      "prediction": "explored nature, roasted marshmallows, and went on a hike",
      "reference": "explored nature, roasted marshmallows, and went on a hike",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.000000238418579,
        "bert_recall": 1.000000238418579,
        "bert_f1": 1.000000238418579,
        "meteor": 0.9993141289437586,
        "sbert_similarity": 0.9999999403953552
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What kind of counseling and mental health services is Melanie interested in pursuing?",
      "prediction": "Not mentioned in the conversation",
      "reference": "working with trans people, helping them accept themselves and supporting their mental health",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8718171119689941,
        "bert_recall": 0.8251791596412659,
        "bert_f1": 0.8478572964668274,
        "meteor": 0.0,
        "sbert_similarity": 0.05818866193294525
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What kind of counseling workshop did Melanie attend recently?",
      "prediction": "Not mentioned in the conversation",
      "reference": "LGBTQ+ counseling workshop",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8808150887489319,
        "bert_recall": 0.8225635886192322,
        "bert_f1": 0.8506932854652405,
        "meteor": 0.0,
        "sbert_similarity": 0.11074962466955185
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What motivated Melanie to pursue counseling?",
      "prediction": "her own journey and the support she received, and how counseling improved her life",
      "reference": "her own journey and the support she received, and how counseling improved her life",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.000000238418579,
        "bert_recall": 1.000000238418579,
        "bert_f1": 1.000000238418579,
        "meteor": 0.9998177842565598,
        "sbert_similarity": 1.0000001192092896
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What kind of place does Melanie want to create for people?",
      "prediction": "a safe and inviting place for people to grow",
      "reference": "a safe and inviting place for people to grow",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 0.9999998807907104,
        "bert_recall": 0.9999998807907104,
        "bert_f1": 0.9999998807907104,
        "meteor": 0.9993141289437586,
        "sbert_similarity": 1.0000001192092896
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "Did Caroline make the black and white bowl in the photo?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Yes",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8480530977249146,
        "bert_recall": 0.8904045820236206,
        "bert_f1": 0.8687129616737366,
        "meteor": 0.0,
        "sbert_similarity": 0.11724153161048889
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What are the new shoes that Caroline got used for?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Running",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8501412868499756,
        "bert_recall": 0.8962053060531616,
        "bert_f1": 0.8725658059120178,
        "meteor": 0.0,
        "sbert_similarity": 0.1654406487941742
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What is Caroline's reason for getting into running?",
      "prediction": "Not mentioned in the conversation",
      "reference": "To de-stress and clear her mind",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8849431872367859,
        "bert_recall": 0.8652690649032593,
        "bert_f1": 0.8749955892562866,
        "meteor": 0.0,
        "sbert_similarity": 0.15259532630443573
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What does Caroline say running has been great for?",
      "prediction": "Her mental health",
      "reference": "Her mental health",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What did Melanie see at the council meeting for adoption?",
      "prediction": "many people wanting to create loving homes for children in need",
      "reference": "many people wanting to create loving homes for children in need",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 0.9999997019767761,
        "bert_recall": 0.9999997019767761,
        "bert_f1": 0.9999997019767761,
        "meteor": 0.9996243425995492,
        "sbert_similarity": 1.0
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What inspired Melanie's painting for the art show?",
      "prediction": "visiting an LGBTQ center and wanting to capture unity and strength",
      "reference": "visiting an LGBTQ center and wanting to capture unity and strength",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9996243425995492,
        "sbert_similarity": 1.0
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What inspired Caroline's sculpture for the art show?",
      "prediction": "visiting an LGBTQ center and wanting to capture unity and strength",
      "reference": "visiting an LGBTQ center and wanting to capture unity and strength",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9996243425995492,
        "sbert_similarity": 1.0
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "How often does Caroline go to the beach with her kids?",
      "prediction": "Not mentioned in the conversation",
      "reference": "once or twice a year",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8639419078826904,
        "bert_recall": 0.8075608611106873,
        "bert_f1": 0.8348004817962646,
        "meteor": 0.0,
        "sbert_similarity": 0.05305670201778412
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What did Caroline and her family see during their camping trip last year?",
      "prediction": "Perseid meteor shower",
      "reference": "Perseid meteor shower",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.000000238418579,
        "bert_recall": 1.000000238418579,
        "bert_f1": 1.000000238418579,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "How did Caroline feel while watching the meteor shower?",
      "prediction": "in awe of the universe",
      "reference": "in awe of the universe",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0000001192092896,
        "bert_recall": 1.0000001192092896,
        "bert_f1": 1.0000001192092896,
        "meteor": 0.996,
        "sbert_similarity": 1.0
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "Why did Caroline choose to use colors and patterns in her pottery project?",
      "prediction": "She wanted to catch the eye and make people smile.",
      "reference": "She wanted to catch the eye and make people smile.",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 0.9999998807907104,
        "bert_recall": 0.9999998807907104,
        "bert_f1": 0.9999998807907104,
        "meteor": 0.9995,
        "sbert_similarity": 1.0
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "Is Oscar Melanie's pet?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Yes",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8480530977249146,
        "bert_recall": 0.8904045820236206,
        "bert_f1": 0.8687129616737366,
        "meteor": 0.0,
        "sbert_similarity": 0.11724153161048889
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "Where did Oscar hide his bone once?",
      "prediction": "In Melanie's slipper",
      "reference": "In Melanie's slipper",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0000001192092896,
        "bert_recall": 1.0000001192092896,
        "bert_f1": 1.0000001192092896,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 0.9999999403953552
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What activity did Melanie used to do with her dad?",
      "prediction": "Horseback riding",
      "reference": "Horseback riding",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.000000238418579,
        "bert_recall": 1.000000238418579,
        "bert_f1": 1.000000238418579,
        "meteor": 0.9375,
        "sbert_similarity": 1.0000001192092896
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What did Melanie make for a local church?",
      "prediction": "a stained glass window",
      "reference": "a stained glass window",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9921875,
        "sbert_similarity": 1.0
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What did Melanie find in her neighborhood during her walk?",
      "prediction": "a rainbow sidewalk",
      "reference": "a rainbow sidewalk",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "Which song motivates Melanie to be courageous?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Brave by Sara Bareilles",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8708802461624146,
        "bert_recall": 0.8365832567214966,
        "bert_f1": 0.8533872961997986,
        "meteor": 0.0,
        "sbert_similarity": -0.004309711046516895
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What type of instrument does Caroline play?",
      "prediction": "Not mentioned in the conversation",
      "reference": "clarinet and violin",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8583570718765259,
        "bert_recall": 0.7918205857276917,
        "bert_f1": 0.8237473964691162,
        "meteor": 0.0,
        "sbert_similarity": 0.012888383120298386
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "Which classical musicians does Caroline enjoy listening to?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Bach and Mozart",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8735334873199463,
        "bert_recall": 0.8279000520706177,
        "bert_f1": 0.850104808807373,
        "meteor": 0.0,
        "sbert_similarity": 0.10292421281337738
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "Who is Caroline a fan of in terms of modern music?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Ed Sheeran",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8636482954025269,
        "bert_recall": 0.734740138053894,
        "bert_f1": 0.7939960956573486,
        "meteor": 0.0,
        "sbert_similarity": 0.0960879698395729
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What precautionary sign did Caroline see at the caf\u00e9?",
      "prediction": "A sign stating that someone is not being able to leave",
      "reference": "A sign stating that someone is not being able to leave",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9996243425995492,
        "sbert_similarity": 1.0
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What setback did Caroline face recently?",
      "prediction": "Not mentioned in the conversation",
      "reference": "She got hurt and had to take a break from pottery.",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8869463801383972,
        "bert_recall": 0.8621917963027954,
        "bert_f1": 0.8743939399719238,
        "meteor": 0.0,
        "sbert_similarity": 0.03774178400635719
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What does Caroline do to keep herself busy during her pottery break?",
      "prediction": "Read a book and paint.",
      "reference": "Read a book and paint.",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.996,
        "sbert_similarity": 1.0
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What was the poetry reading that Melanie attended about?",
      "prediction": "It was a transgender poetry reading where transgender people shared their stories.",
      "reference": "It was a transgender poetry reading where transgender people shared their stories.",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0000001192092896,
        "bert_recall": 1.0000001192092896,
        "bert_f1": 1.0000001192092896,
        "meteor": 0.9997106481481481,
        "sbert_similarity": 1.0000001192092896
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What happened to Caroline's son on their road trip?",
      "prediction": "He got into an accident",
      "reference": "He got into an accident",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 0.9999998807907104,
        "bert_recall": 0.9999998807907104,
        "bert_f1": 0.9999998807907104,
        "meteor": 0.996,
        "sbert_similarity": 1.0
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "How did Caroline's son handle the accident?",
      "prediction": "He was scared but reassured by his family",
      "reference": "He was scared but reassured by his family",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9990234375,
        "sbert_similarity": 1.0
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "How did Caroline feel about her family after the accident?",
      "prediction": "They are important and mean the world to her",
      "reference": "They are important and mean the world to her",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 0.9999998807907104,
        "bert_recall": 0.9999998807907104,
        "bert_f1": 0.9999998807907104,
        "meteor": 0.9993141289437586,
        "sbert_similarity": 1.0
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "How did Caroline's children handle the accident?",
      "prediction": "They were scared but resilient",
      "reference": "They were scared but resilient",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.000000238418579,
        "bert_recall": 1.000000238418579,
        "bert_f1": 1.000000238418579,
        "meteor": 0.996,
        "sbert_similarity": 1.0
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "How did Caroline feel after the accident?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Grateful and thankful for her family",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8705782294273376,
        "bert_recall": 0.819611668586731,
        "bert_f1": 0.8443265557289124,
        "meteor": 0.0,
        "sbert_similarity": 0.012023916468024254
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What was Caroline's reaction to her children enjoying the Grand Canyon?",
      "prediction": "She was happy and thankful",
      "reference": "She was happy and thankful",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.000000238418579,
        "bert_recall": 1.000000238418579,
        "bert_f1": 1.000000238418579,
        "meteor": 0.996,
        "sbert_similarity": 1.0000001192092896
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What did Caroline do after the road trip to relax?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Went on a nature walk or hike",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8826772570610046,
        "bert_recall": 0.8326032757759094,
        "bert_f1": 0.8569093346595764,
        "meteor": 0.0,
        "sbert_similarity": 0.0687059611082077
      },
      "context_tokens": 17900,
      "compressions": 0
    },
    {
      "sample_id": 0,
      "question": "What does Caroline love most about camping with her family?",
      "prediction": "Being present and bonding with her family",
      "reference": "Being present and bonding with her family",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 0.9999998211860657,
        "bert_recall": 0.9999998211860657,
        "bert_f1": 0.9999998211860657,
        "meteor": 0.9985422740524781,
        "sbert_similarity": 0.9999999403953552
      },
      "context_tokens": 17900,
      "compressions": 0
    }
  ],
  "compression_events": 0,
  "avg_context_size": 17900.0,
  "timestamp": "2025-12-12T20:16:55.727470"
}